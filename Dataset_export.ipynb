{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime, time\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "from functools import reduce\n",
    "from pyspark.sql import Window\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import (StructField, StringType,FloatType, \n",
    "                               DoubleType, IntegerType, StructType,\n",
    "                              DateType)\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import DataFrame as spDataFrame\n",
    "\n",
    "\n",
    "from pyspark.ml import Pipeline, PipelineModel\n",
    "from pyspark.ml.feature import (Binarizer, OneHotEncoder, StringIndexer, HashingTF, \n",
    "                                Tokenizer, StandardScaler, VectorAssembler,\n",
    "                               OneHotEncoder, StringIndexer, VectorIndexer)\n",
    "from pyspark.ml.classification import LogisticRegression, LinearSVC, LinearSVCModel\n",
    "from pyspark.ml.evaluation import HasFeaturesCol\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "#from tokenizer import Tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is to setup the sparksession. If additional resources need to be allocated it is done with this command"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "spark = SparkSession \\\n",
    "        .builder \\\n",
    "        .appName('Capstone Analysis') \\\n",
    "        .config(\"spark.driver.memory\", \"60g\") \\\n",
    "        .config(\"spark.executor.memory\", \"60g\") \\\n",
    "        .config(\"spark.executor.cores\", \"5\") \\\n",
    "        .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.3.2'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "proc_data_schema = [StructField('time',IntegerType(),True),\n",
    "                   StructField('user@domain',StringType(),True),\n",
    "                   StructField('src_comp',StringType(),True),\n",
    "                   StructField('proc_name',StringType(),True),\n",
    "                   StructField('strt',StringType())]\n",
    "proc_final_struc = StructType(fields = proc_data_schema)\n",
    "proc = spark.read.csv('../Dataset/proc.txt',schema=proc_final_struc)\n",
    "\n",
    "auth_data_schema = [StructField('time',IntegerType(),True),\n",
    "                   StructField('src_user@domain',StringType(),True),\n",
    "                   StructField('dest_user@domain',StringType(),True),\n",
    "                   StructField('src_comp',StringType(),True),\n",
    "                   StructField('dest_comp',StringType(),True),\n",
    "                   StructField('auth_type',StringType(),True),\n",
    "                   StructField('logon_type',StringType(),True),\n",
    "                   StructField('auth_orient',StringType(),True),\n",
    "                   StructField('success',StringType(),True)]\n",
    "auth_final_struc = StructType(fields = auth_data_schema)\n",
    "auth = spark.read.csv('../Dataset/auth.txt',schema=auth_final_struc )\n",
    "\n",
    "flows_data_schema = [StructField('time',IntegerType(),False),\n",
    "                   StructField('dur',IntegerType(),False),\n",
    "                   StructField('src_comp',StringType(),False),\n",
    "                   StructField('src_port',StringType(),False),\n",
    "                   StructField('dest_comp',StringType(),False),\n",
    "                   StructField('dest_port',StringType(),False),\n",
    "                   StructField('protocol',StringType(),False),\n",
    "                   StructField('pkt_cnt',IntegerType(),False),\n",
    "                   StructField('byte_cnt',IntegerType(),False)]\n",
    "flows_final_struc = StructType(fields = flows_data_schema)\n",
    "flows = spark.read.csv('../Dataset/flows.txt',schema=flows_final_struc )\n",
    "\n",
    "dns_data_schema = [StructField('time',IntegerType(),True),\n",
    "                   StructField('src_comp',StringType(),True),\n",
    "                   StructField('cmp_resolved',StringType(),True)]\n",
    "dns_final_struc = StructType(fields = dns_data_schema)\n",
    "dns = spark.read.csv('../Dataset/dns.txt',schema=dns_final_struc)\n",
    "\n",
    "redteam_data_schema = [StructField('time',IntegerType(),True),\n",
    "                   StructField('user@domain',StringType(),True),\n",
    "                   StructField('src_comp',StringType(),True),\n",
    "                   StructField('dest_comp',StringType(),True)]\n",
    "redteam_final_struc = StructType(fields = redteam_data_schema)\n",
    "redteam = spark.read.csv('../Dataset/redteam.txt',schema=redteam_final_struc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code is supposed to map the DNS connections into a network map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dns_graph = new_df.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new_dns_graph = dns_graph.sample(5000)\n",
    "# G = nx.from_pandas_edgelist(new_dns_graph, 'src_comp','cmp_resolved','count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from operator import itemgetter\n",
    "# node_and_degree = G.degree()\n",
    "# (largest_hub, degree) = sorted(node_and_degree, key=itemgetter(1))[-1]\n",
    "# hub_ego = nx.ego_graph(G, largest_hub)\n",
    "# # Draw graph\n",
    "# pos = nx.spring_layout(hub_ego)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nx.draw(G,pos,node_color='b', node_size=50, with_labels=False)\n",
    "# nx.draw_networkx_nodes(hub_ego, pos, nodelist=[largest_hub], node_size=300, node_color='r')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transform Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will be where we split all user@domain columns into user and domain columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "proc_split = F.split(proc['user@domain'],'@')\n",
    "proc = proc.withColumn('src_user',proc_split.getItem(0))\n",
    "proc = proc.withColumn('src_dmn',proc_split.getItem(1))\n",
    "proc = proc.drop('user@domain')\n",
    "\n",
    "proc = proc.withColumn('type',F.lit('Process'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "auth_src_split = F.split(auth['src_user@domain'],'@')\n",
    "auth = auth.withColumn('src_user',auth_src_split.getItem(0))\n",
    "auth = auth.withColumn('src_dmn',auth_src_split.getItem(1))\n",
    "\n",
    "auth_dest_split = F.split(auth['dest_user@domain'],'@')\n",
    "auth = auth.withColumn('dest_user',auth_dest_split.getItem(0))\n",
    "auth = auth.withColumn('dest_dmn',auth_dest_split.getItem(1))\n",
    "\n",
    "auth = auth.drop('src_user@domain','dest_user@domain')\n",
    "\n",
    "auth = auth.withColumn('type',F.lit('Auth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "redteam_split = F.split(redteam['user@domain'],'@')\n",
    "redteam = redteam.withColumn('src_user',redteam_split.getItem(0))\n",
    "redteam = redteam.withColumn('src_dmn',redteam_split.getItem(1))\n",
    "\n",
    "redteam = redteam.drop('user@domain')\n",
    "\n",
    "redteam = redteam.withColumn('type',F.lit('RedTeam'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "flows = flows.withColumn('avg_pkt_size', (flows['byte_cnt']/flows['pkt_cnt']).cast(DoubleType()))\n",
    "flows = flows.na.drop(how='all')\n",
    "\n",
    "flows = flows.withColumn('type',F.lit('DataFlow'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test/Train split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_days = 35\n",
    "split_range = split_days * 3600 * 24"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Split data into train/test segments. This will be done on the first number of days\n",
    "\n",
    "# redteam = redteam.filter(redteam.time <= split_range)\n",
    "\n",
    "# dns = dns.filter(dns.time <= split_range)\n",
    "\n",
    "# proc = proc.filter(proc.time <= split_range)\n",
    "\n",
    "# flows = flows.filter(flows.time <= split_range)\n",
    "\n",
    "# auth = auth.filter(auth.time <= split_range)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# days = 20\n",
    "# train_range = days * 3600 * 24"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Split data into train/test segments. This will be done on the first number of days\n",
    "\n",
    "# redteam_test = redteam.filter(redteam.time > train_range)\n",
    "# redteam_train = redteam.filter(redteam.time <= train_range)\n",
    "\n",
    "# dns_test = dns.filter(dns.time > train_range)\n",
    "# dns_train = dns.filter(dns.time <= train_range)\n",
    "\n",
    "# proc_test = proc.filter(proc.time > train_range)\n",
    "# proc_train = proc.filter(proc.time <= train_range)\n",
    "\n",
    "# flows_test = flows.filter(flows.time > train_range)\n",
    "# flows_train = flows.filter(flows.time <= train_range)\n",
    "\n",
    "# auth_test = auth.filter(auth.time > train_range)\n",
    "# auth_train = auth.filter(auth.time <= train_range)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# colum = proc.columns\n",
    "# colum.sort()\n",
    "\n",
    "# proc = proc.select(colum)\n",
    "# redteam = redteam.select(colum)\n",
    "# auth = auth.select(colum)\n",
    "# flows = flows.select(colum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# master = unionAll(redteam,auth,proc,flows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(master.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# master.select('strt').sort('strt').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code was supposed to transform the time column from an int into a datetime data type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# redteam1 = redteam.select(F.to_date(redteam.time,'MM-dd HH:mm:ss').alias('date')).collect()\n",
    "#redteam1 = redteam.rdd.map(lambda x: (x['time'], time.strftime('%m/%d %H:%M:%S', time.gmtime(x['time']) ))).toDF(['time','timestam'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DNS Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dns_train.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# F.pandas_udf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dns_extract(dns_dataset):\n",
    "    dns_count = dns_dataset.groupby(dns_dataset.time,dns_dataset.src_comp).count().na.fill(0).sort('time')\n",
    "    dns_count = dns_count.select(F.col('time'),F.col('src_comp'),F.col('count').alias('dns_count'))\n",
    "    \n",
    "    final_dns_dataset = dns_count\n",
    "    \n",
    "    temp_dns = first_dns_extract(dns_dataset)\n",
    "    \n",
    "    final_dns_dataset = final_dns_dataset.join(temp_dns,['time','src_comp'],'left')\n",
    "    \n",
    "    final_dns_dataset = final_dns_dataset.na.fill(0)\n",
    "    \n",
    "    final_dns_dataset = final_dns_dataset.sort(['time','src_comp'])\n",
    "    \n",
    "    return final_dns_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def first_dns_extract(dns_dataset):\n",
    "    newresolved = dns_dataset.groupby(dns_dataset.src_comp,dns_dataset.cmp_resolved).agg(F.min('time').alias('time')).sort('time')\n",
    "    newresolved = newresolved.repartition(10).groupby(newresolved.time,newresolved.src_comp).count()\n",
    "    newresolved = newresolved.repartition(10).select(F.col('time'),F.col('src_comp'),F.col('count').alias('newresolved_count')).sort('time')\n",
    "    \n",
    "    first_dns_dataset = newresolved\n",
    "    return first_dns_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "dns_sample = dns_extract(dns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 78.87155055999756 seconds ---\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "dns_sample.coalesce(1).write.csv('../Dataset/Output/total/dns_output_1.csv')\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start_time = time.time()\n",
    "\n",
    "# dns_sample.count()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Proc Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# proc_train.select('strt').distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# proc_subset = proc_train.filter((proc_train.time<2)&(proc_train.time>=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# proc_subset.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#proc_subset_test = proc_subset.groupby(proc_subset.time,proc_subset.src_comp,proc_subset.proc_name,proc_subset.strt).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# proc_subset_test.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# proc_subset_test = proc_subset.groupby(proc_subset.time,proc_subset.src_comp)\\\n",
    "#                     .agg(F.sum(F.when(proc_subset.strt=='Start',F.lit(1)).otherwise(F.lit(-1))).alias('LoggedOn'))\\\n",
    "#                     .withColumn('LoggedOn',F.greatest(F.lit(0),'LoggedOn'))\\\n",
    "#                     .sort('time','src_comp')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# proc_subset_test.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# proc_count = proc_train.groupby(proc_train.time,proc_train.src_comp).count().na.fill(0).sort('time')\n",
    "# proc_count = proc_count.select(F.col('time'),F.col('src_comp'),F.col('count').alias('proc_total'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# proc_count.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# proc_count.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# proc_exec = proc_train.filter(proc_train.strt=='Start').groupby(proc_train.time,proc_train.src_comp).count().na.fill(0).sort('time')\n",
    "# proc_exec = proc_exec.select(F.col('time'),F.col('src_comp'),F.col('count').alias('proc_exec_total'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# proc_exec.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# proc_exec.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def first_proc_extract(proc_dataset):\n",
    "    newexecute = proc_dataset.filter(proc_dataset.strt=='Start').groupby(proc_dataset.src_comp,proc_dataset.proc_name).agg(F.min('time').alias('time')).sort('time')\n",
    "    newexecute = newexecute.groupby(newexecute.time,newexecute.src_comp).count()\n",
    "    newexecute = newexecute.select(F.col('time'),F.col('src_comp'),F.col('count').alias('newexecute_count')).sort('time')\n",
    "    \n",
    "    first_execute_dataset = newexecute\n",
    "    return first_execute_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def proc_running_extract(proc_dataset):\n",
    "    proc_running = proc_dataset.groupby(proc_dataset.time,proc_dataset.src_comp)\\\n",
    "                    .agg(F.sum(F.when(proc_dataset.strt=='Start',F.lit(1)).otherwise(F.lit(-1))).alias('Proc_run'))\\\n",
    "                    .withColumn('Proc_run',F.greatest(F.lit(0),'Proc_run'))\\\n",
    "                    .sort('time','src_comp')\n",
    "    return proc_running\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def proc_extract(proc_dataset):\n",
    "    proc_count = proc_dataset.groupby(proc_dataset.time,proc_dataset.src_comp).count().na.fill(0).sort('time')\n",
    "    proc_count = proc_count.select(F.col('time'),F.col('src_comp'),F.col('count').alias('proc_total'))\n",
    "    \n",
    "    proc_exec = proc_dataset.filter(proc_dataset.strt=='Start').groupby(proc_dataset.time,proc_dataset.src_comp).count().na.fill(0).sort('time')\n",
    "    proc_exec = proc_exec.select(F.col('time'),F.col('src_comp'),F.col('count').alias('proc_exec_total'))\n",
    "    \n",
    "    proc_running = proc_dataset.groupby(proc_dataset.time,proc_dataset.src_comp)\\\n",
    "                    .agg(F.sum(F.when(proc_dataset.strt=='Start',F.lit(1)).otherwise(F.lit(-1))).alias('Proc_run'))\\\n",
    "                    .withColumn('Proc_run',F.greatest(F.lit(0),'Proc_run'))\\\n",
    "                    .sort('time','src_comp')\n",
    "    \n",
    "    first_execute = first_proc_extract(proc_dataset)\n",
    "    \n",
    "    \n",
    "    final_proc_dataset=proc_count\n",
    "    \n",
    "    final_proc_dataset = final_proc_dataset.join(proc_exec,['time','src_comp'],'left')\n",
    "    \n",
    "    final_proc_dataset = final_proc_dataset.join(first_execute,['time','src_comp'],'left')\n",
    "    \n",
    "    final_proc_dataset = final_proc_dataset.join(proc_running,['time','src_comp'],'left')\n",
    "    \n",
    "    final_proc_dataset = final_proc_dataset.na.fill(0)\n",
    "    \n",
    "    final_proc_dataset = final_proc_dataset.sort(['time','src_comp'])\n",
    "    \n",
    "    \n",
    "    return final_proc_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "proc_sample = proc_extract(proc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 721.9334058761597 seconds ---\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "proc_sample.coalesce(1).write.csv('../Dataset/Output/total/proc_output_1')\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Auth Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def auth_extract(auth_dataset):\n",
    "    failed_logon = auth_dataset.filter(auth_dataset.success=='Fail').groupby(auth_dataset.time,auth_dataset.src_comp).count().na.fill(0).sort('time')\n",
    "    failed_logon = failed_logon.select(F.col('time'),F.col('src_comp'),F.col('count').alias('fail_count'))\n",
    "    \n",
    "    \n",
    "    final_auth_dataset = failed_logon\n",
    "    return final_auth_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def auth_loggedon_extract(auth_dataset):\n",
    "    auth_loggedon = auth_dataset.filter((auth_dataset.success==\"Success\") & ((auth_dataset.auth_orient=='LogOn')|(auth_dataset.auth_orient=='LogOff'))).groupby(auth_dataset.time,auth_dataset.src_comp)\\\n",
    "                    .agg(F.sum(F.when(auth_dataset.auth_orient=='LogOn',F.lit(1)).otherwise(F.lit(-1))).alias('LoggedOn'))\\\n",
    "                    .withColumn('LoggedOn',F.greatest(F.lit(0),'LoggedOn'))\\\n",
    "                    .sort('time','src_comp')\n",
    "    return auth_loggedon\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def joined_auth_extract(auth_dataset):\n",
    "\n",
    "    failed_logon = auth_dataset.filter(auth_dataset.success=='Fail').groupby(auth_dataset.time,auth_dataset.src_comp).count().na.fill(0).sort('time')\n",
    "    failed_logon = failed_logon.select(F.col('time'),F.col('src_comp'),F.col('count').alias('fail_count'))\n",
    "    \n",
    "    auth_loggedon = auth_loggedon_extract(auth_dataset)\n",
    "    \n",
    "    final_auth_dataset = failed_logon.join(auth_loggedon,['time','src_comp'],'outer' )\n",
    "    return final_auth_dataset\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "auth_sample = joined_auth_extract(auth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 1084.1272368431091 seconds ---\n"
     ]
    }
   ],
   "source": [
    "auth_sample.coalesce(1).write.csv('../Dataset/Output/total/auth_output_1.csv')\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Redteam Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def redteam_extract(redteam_dataset):\n",
    "    redteam_event = redteam_dataset.groupby(redteam_dataset.time, redteam_dataset.src_comp).count().na.fill(0).sort('time')\n",
    "    redteam_event = redteam_event.select(F.col('time'),F.col('src_comp'),F.col('count').alias('redteam_event'))\n",
    "    \n",
    "    final_redteam_dataset = redteam_event\n",
    "    return final_redteam_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 1.6539995670318604 seconds ---\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "redteam_sample = redteam_extract(redteam)\n",
    "redteam_sample.coalesce(1).write.csv('../Dataset/Output/total/redteam_output_1.csv')\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Flows Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# flows_train.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# flows_data = flows_train.select('time','dur','src_comp','pkt_cnt','byte_cnt','avg_pkt_size')\\\n",
    "#                         .groupby('time','src_comp').agg(F.sum('dur').alias('dur'),F.sum('pkt_cnt').alias('pkt_cnt'),F.sum('byte_cnt').alias('byte_cnt'),F.avg('avg_pkt_size').alias('avg_pkt_size')).sort('time')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# flows_data.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flows_extract(flows_dataset):\n",
    "    flows_data = flows_dataset.select('time','dur','src_comp','pkt_cnt','byte_cnt','avg_pkt_size')\\\n",
    "                        .groupby('time','src_comp').agg(F.sum('dur').alias('dur'),F.sum('pkt_cnt').alias('pkt_cnt'),F.sum('byte_cnt').alias('byte_cnt'),F.avg('avg_pkt_size').alias('avg_pkt_size')).sort('time')\n",
    "    \n",
    "    final_flows_dataset = flows_data\n",
    "    return final_flows_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "flows_sample = flows_extract(flows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "# flows_sample.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 120.35145854949951 seconds ---\n"
     ]
    }
   ],
   "source": [
    "flows_sample.coalesce(1).write.csv('../Dataset/Output/total/flows_output_1.csv')\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combine Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "redteam_columns = ['time', 'src_comp', 'redteam_event']\n",
    "auth_columns = ['time', 'src_comp', 'fail_count', 'LoggedOn']\n",
    "dns_columns = ['time', 'src_comp', 'dns_count', 'newresolved_count']\n",
    "proc_columns = ['time', 'src_comp', 'proc_total', 'proc_exec_total', 'newexecute_count', 'Proc_run']\n",
    "flows_columns = ['time', 'src_comp', 'dur', 'pkt_cnt', 'byte_cnt', 'avg_pkt_size']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imported  ../Dataset/Output/total/dns_output_1.csv/part-00000-30c6be83-d671-4851-a9db-d44cb889d133-c000.csv\n",
      "imported  ../Dataset/Output/total/proc_output_1/part-00000-df051b86-70c7-4781-9e2a-b28b227687a4-c000.csv\n",
      "imported  ../Dataset/Output/total/auth_output_1.csv/part-00000-2efabf19-3e9d-4f43-a5d2-d702da7acbf3-c000.csv\n",
      "imported  ../Dataset/Output/total/redteam_output_1.csv/part-00000-4ff2c392-5a39-4beb-bbe7-0db94a344c11-c000.csv\n",
      "imported  ../Dataset/Output/total/flows_output_1.csv/part-00000-406f70d5-1b32-4b26-a26c-1bae6d6a5343-c000.csv\n"
     ]
    }
   ],
   "source": [
    "directory = \"../Dataset/Output/total/\"\n",
    "for root, dirs, files in os.walk(directory):\n",
    "    for file in files:\n",
    "        if file.endswith('.csv'):\n",
    "            filepath = root+'/'+file\n",
    "            if \"dns\" in root:   \n",
    "                pd_dns_df = pd.read_csv(filepath, names=dns_columns)\n",
    "                print(\"imported \", filepath)\n",
    "            elif \"proc\" in root:\n",
    "                pd_proc_df = pd.read_csv(filepath, names=proc_columns)\n",
    "                print(\"imported \", filepath)\n",
    "            elif \"flows\" in root:   \n",
    "                pd_flows_df = pd.read_csv(filepath, names=flows_columns)\n",
    "                print(\"imported \", filepath)\n",
    "            elif \"redteam\" in root:\n",
    "                pd_redteam_df = pd.read_csv(filepath, names=redteam_columns)\n",
    "                print(\"imported \", filepath)\n",
    "            elif \"auth\" in root:\n",
    "                pd_auth_df = pd.read_csv(filepath, names=auth_columns)\n",
    "                print(\"imported \", filepath)\n",
    "            else:\n",
    "                print(\"Error importing \", file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # file = '../Dataset/Output/complete_df.csv'\n",
    "\n",
    "# # start_time = time.time()\n",
    "\n",
    "\n",
    "# n_size = pd_auth_df.time.max()\n",
    "\n",
    "# splits = 15\n",
    "# switch = 0\n",
    "\n",
    "# for n in range(int(splits)):\n",
    "\n",
    "#     lower = int(n_size/splits*n)\n",
    "#     upper = int(n_size/splits*(n+1))\n",
    "    \n",
    "#     redteam = pd_redteam_df[(pd_redteam_df.time > lower) & (pd_redteam_df.time <= upper)]\n",
    "\n",
    "#     dns = pd_dns_df[(pd_dns_df.time > lower) & (pd_dns_df.time <= upper)]\n",
    "\n",
    "#     proc = pd_proc_df[(pd_proc_df.time > lower) & (pd_proc_df.time <= upper)]\n",
    "\n",
    "#     flows = pd_flows_df[(pd_flows_df.time > lower) & (pd_flows_df.time <= upper)]\n",
    "\n",
    "#     auth = pd_auth_df[(pd_auth_df.time > lower) & (pd_auth_df.time <= upper)]\n",
    "\n",
    "# #     master_df = dns.merge(auth,on=['time','src_comp'],how='outer')\n",
    "# #     master_df = master_df.merge(flows,on=['time','src_comp'],how='outer')\n",
    "# #     master_df = master_df.merge(redteam,on=['time','src_comp'],how='outer')\n",
    "# #     master_df = master_df.merge(proc,on=['time','src_comp'],how='outer')\n",
    "\n",
    "# #     master_df=master_df.fillna(0)\n",
    "    \n",
    "# # #     master_df['time'] = pd.to_datetime(master_df['time'],unit='s').dt.strftime('%m/%d %H:%M:%S').head()\n",
    "\n",
    "#     if switch == 0:\n",
    "#         master_df.to_csv(file, index=False)\n",
    "#         switch = 1 \n",
    "#     else:\n",
    "#         master_df.to_csv(file,mode='a',header=False, index=False )\n",
    "#     print('Done with round ', (n+1),' of ',(splits), flush=True)    \n",
    "\n",
    "# # print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This block will save a csv with all data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/es7/lib/python3.6/site-packages/ipykernel_launcher.py:26: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/usr/local/es7/lib/python3.6/site-packages/ipykernel_launcher.py:27: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/usr/local/es7/lib/python3.6/site-packages/ipykernel_launcher.py:28: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/usr/local/es7/lib/python3.6/site-packages/ipykernel_launcher.py:29: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/usr/local/es7/lib/python3.6/site-packages/ipykernel_launcher.py:30: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done with round  1  of  15\n",
      "--- 3423.0238881111145 seconds ---\n",
      "Done with round  2  of  15\n",
      "--- 7271.482785224915 seconds ---\n",
      "Done with round  3  of  15\n",
      "--- 11304.832357645035 seconds ---\n",
      "Done with round  4  of  15\n",
      "--- 15332.420239210129 seconds ---\n",
      "Done with round  5  of  15\n",
      "--- 18188.518383979797 seconds ---\n",
      "Done with round  6  of  15\n",
      "--- 22211.44558262825 seconds ---\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "file = '../Dataset/Output/complete_1m_df.csv'\n",
    "\n",
    "splits = 15\n",
    "\n",
    "n_size = pd_auth_df.time.max()\n",
    "\n",
    "splits = 15\n",
    "switch = 0\n",
    "\n",
    "for n in range(int(splits)):\n",
    "\n",
    "    lower = int(n_size/splits*n)\n",
    "    upper = int(n_size/splits*(n+1))\n",
    "    \n",
    "    redteam = pd_redteam_df[(pd_redteam_df.time > lower) & (pd_redteam_df.time <= upper)]\n",
    "\n",
    "    dns = pd_dns_df[(pd_dns_df.time > lower) & (pd_dns_df.time <= upper)]\n",
    "\n",
    "    proc = pd_proc_df[(pd_proc_df.time > lower) & (pd_proc_df.time <= upper)]\n",
    "\n",
    "    flows = pd_flows_df[(pd_flows_df.time > lower) & (pd_flows_df.time <= upper)]\n",
    "\n",
    "    auth = pd_auth_df[(pd_auth_df.time > lower) & (pd_auth_df.time <= upper)]\n",
    "\n",
    "    dns['time'] = pd.to_datetime(dns['time'], unit='s').apply(lambda dt: dt.replace(year=2017))\n",
    "    flows['time'] = pd.to_datetime(flows['time'], unit='s').apply(lambda dt: dt.replace(year=2017))\n",
    "    auth['time'] = pd.to_datetime(auth['time'], unit='s').apply(lambda dt: dt.replace(year=2017))\n",
    "    redteam['time'] = pd.to_datetime(redteam['time'], unit='s').apply(lambda dt: dt.replace(year=2017))\n",
    "    proc['time'] = pd.to_datetime(proc['time'], unit='s').apply(lambda dt: dt.replace(year=2017))\n",
    "\n",
    "    resample_pd_dns_df = dns.set_index('time').groupby('src_comp').resample('1min').agg({\n",
    "                                                                           'dns_count':'sum',\n",
    "                                                                           'newresolved_count':'sum'} ).reset_index().fillna(-999)\n",
    "    resample_pd_flows_df = flows.set_index('time').groupby('src_comp').resample('1min').agg({\n",
    "                                                                           'dur':'sum',\n",
    "                                                                           'pkt_cnt':'sum',\n",
    "                                                                           'byte_cnt':'sum',\n",
    "                                                                           'avg_pkt_size':'mean'} ).reset_index().fillna(-999)\n",
    "    resample_pd_auth_df = auth.set_index('time').groupby('src_comp').resample('1min').agg({\n",
    "                                                                           'fail_count':'sum',\n",
    "                                                                           'LoggedOn':'sum'} ).reset_index().fillna(-999)\n",
    "    resample_pd_redteam_df = redteam.set_index('time').groupby('src_comp').resample('1min').agg({'redteam_event':'sum'\n",
    "                                                                        } ).reset_index().fillna(-999)\n",
    "    resample_pd_proc_df = proc.set_index('time').groupby('src_comp').resample('1min').agg({\n",
    "                                                                           'proc_total':'sum',\n",
    "                                                                           'proc_exec_total':'sum',\n",
    "                                                                           'newexecute_count':'sum',\n",
    "                                                                           'Proc_run':'sum'} ).reset_index().fillna(-999)\n",
    "\n",
    "    master_1m_df = resample_pd_auth_df.merge(resample_pd_dns_df,on=['time','src_comp'],how='outer')\n",
    "    master_1m_df = master_1m_df.merge(resample_pd_flows_df,on=['time','src_comp'],how='outer')\n",
    "    master_1m_df = master_1m_df.merge(resample_pd_redteam_df,on=['time','src_comp'],how='outer')\n",
    "    master_1m_df = master_1m_df.merge(resample_pd_proc_df,on=['time','src_comp'],how='outer')\n",
    "\n",
    "    master_1m_df=master_1m_df.fillna(0)\n",
    "    master_1m_df = master_1m_df.replace(-999, 0)\n",
    "\n",
    "    resampled_df = master_1m_df.groupby('time').agg({'redteam_event':'sum',\n",
    "                                                   'dns_count':'mean',\n",
    "                                                   'newresolved_count':'mean',\n",
    "                                                   'fail_count':'mean',\n",
    "                                                   'LoggedOn':'mean',\n",
    "                                                   'dur':'mean',\n",
    "                                                   'pkt_cnt':'mean',\n",
    "                                                   'byte_cnt':'mean',\n",
    "                                                   'avg_pkt_size':'mean',\n",
    "                                                   'proc_total':'mean',\n",
    "                                                   'proc_exec_total':'mean',\n",
    "                                                   'newexecute_count':'mean',\n",
    "                                                   'Proc_run':'mean'} )\n",
    "    #resampled_df.index = pd.to_datetime(resampled_df.index)\n",
    "    if switch == 0:\n",
    "        master_1m_df.to_csv(file)\n",
    "        switch = 1 \n",
    "    else:\n",
    "        master_1m_df.to_csv(file,mode='a',header=False)\n",
    "    print('Done with round ', (n+1),' of ',(splits), flush=True)    \n",
    "\n",
    "    print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 5004.983376026154 seconds ---\n"
     ]
    }
   ],
   "source": [
    "# start_time = time.time()\n",
    "# file = '../Dataset/Output/complete_5m_df.csv'\n",
    "\n",
    "# splits = 15\n",
    "\n",
    "# pd_dns_df['time'] = pd.to_datetime(pd_dns_df['time'], unit='s').apply(lambda dt: dt.replace(year=2017))\n",
    "# pd_flows_df['time'] = pd.to_datetime(pd_flows_df['time'], unit='s').apply(lambda dt: dt.replace(year=2017))\n",
    "# pd_auth_df['time'] = pd.to_datetime(pd_auth_df['time'], unit='s').apply(lambda dt: dt.replace(year=2017))\n",
    "# pd_redteam_df['time'] = pd.to_datetime(pd_redteam_df['time'], unit='s').apply(lambda dt: dt.replace(year=2017))\n",
    "# pd_proc_df['time'] = pd.to_datetime(pd_proc_df['time'], unit='s').apply(lambda dt: dt.replace(year=2017))\n",
    "\n",
    "# resample_pd_dns_df = pd_dns_df.set_index('time').groupby('src_comp').resample('5min').agg({\n",
    "#                                                                        'dns_count':'sum',\n",
    "#                                                                        'newresolved_count':'sum'} ).reset_index().fillna(-999)\n",
    "# resample_pd_flows_df = pd_flows_df.set_index('time').groupby('src_comp').resample('5min').agg({\n",
    "#                                                                        'dur':'sum',\n",
    "#                                                                        'pkt_cnt':'sum',\n",
    "#                                                                        'byte_cnt':'sum',\n",
    "#                                                                        'avg_pkt_size':'mean'} ).reset_index().fillna(-999)\n",
    "# resample_pd_auth_df = pd_auth_df.set_index('time').groupby('src_comp').resample('5min').agg({\n",
    "#                                                                        'fail_count':'sum',\n",
    "#                                                                        'LoggedOn':'sum'} ).reset_index().fillna(-999)\n",
    "# resample_pd_redteam_df = pd_redteam_df.set_index('time').groupby('src_comp').resample('5min').agg({'redteam_event':'sum'\n",
    "#                                                                     } ).reset_index().fillna(-999)\n",
    "# resample_pd_proc_df = pd_proc_df.set_index('time').groupby('src_comp').resample('5min').agg({\n",
    "#                                                                        'proc_total':'sum',\n",
    "#                                                                        'proc_exec_total':'sum',\n",
    "#                                                                        'newexecute_count':'sum',\n",
    "#                                                                        'Proc_run':'sum'} ).reset_index().fillna(-999)\n",
    "\n",
    "# master_5m_df = resample_pd_auth_df.merge(resample_pd_dns_df,on=['time','src_comp'],how='outer')\n",
    "# master_5m_df = master_5m_df.merge(resample_pd_flows_df,on=['time','src_comp'],how='outer')\n",
    "# master_5m_df = master_5m_df.merge(resample_pd_redteam_df,on=['time','src_comp'],how='outer')\n",
    "# master_5m_df = master_5m_df.merge(resample_pd_proc_df,on=['time','src_comp'],how='outer')\n",
    "\n",
    "# master_5m_df=master_5m_df.fillna(-999)\n",
    "\n",
    "# master_5m_df.to_csv(file, index=False)\n",
    "# print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stop running up to here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Classification model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = StratifiedKFold(n_splits=6)\n",
    "classifier = xgb.XGBClassifier()\n",
    "\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n",
    "\n",
    "\n",
    "tprs = []\n",
    "aucs = []\n",
    "mean_fpr = np.linspace(0, 1, 100)\n",
    "\n",
    "i = 0\n",
    "for train, test in cv.split(X, y):\n",
    "    print('round ',i+1)\n",
    "    probas_ = classifier.fit(X[train], y[train]).predict_proba(X[test])\n",
    "    # Compute ROC curve and area the curve\n",
    "    fpr, tpr, thresholds = roc_curve(y[test], probas_[:, 1])\n",
    "    tprs.append(interp(mean_fpr, fpr, tpr))\n",
    "    tprs[-1][0] = 0.0\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    aucs.append(roc_auc)\n",
    "    plt.plot(fpr, tpr, lw=1, alpha=0.3,\n",
    "             label='ROC fold %d (AUC = %0.2f)' % (i, roc_auc))\n",
    "\n",
    "    i += 1\n",
    "plt.plot([0, 1], [0, 1], linestyle='--', lw=2, color='r',\n",
    "         label='Chance', alpha=.8)\n",
    "\n",
    "mean_tpr = np.mean(tprs, axis=0)\n",
    "mean_tpr[-1] = 1.0\n",
    "mean_auc = auc(mean_fpr, mean_tpr)\n",
    "std_auc = np.std(aucs)\n",
    "plt.plot(mean_fpr, mean_tpr, color='b',\n",
    "         label=r'Mean ROC (AUC = %0.2f $\\pm$ %0.2f)' % (mean_auc, std_auc),\n",
    "         lw=2, alpha=.8)\n",
    "\n",
    "std_tpr = np.std(tprs, axis=0)\n",
    "tprs_upper = np.minimum(mean_tpr + std_tpr, 1)\n",
    "tprs_lower = np.maximum(mean_tpr - std_tpr, 0)\n",
    "plt.fill_between(mean_fpr, tprs_lower, tprs_upper, color='grey', alpha=.2,\n",
    "                 label=r'$\\pm$ 1 std. dev.')\n",
    "\n",
    "plt.xlim([-0.05, 1.05])\n",
    "plt.ylim([-0.05, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver operating characteristic example')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
