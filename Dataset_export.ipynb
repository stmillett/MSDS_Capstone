{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime, time\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "from functools import reduce\n",
    "from pyspark.sql import Window\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import (StructField, StringType,FloatType, \n",
    "                               DoubleType, IntegerType, StructType,\n",
    "                              DateType)\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import DataFrame as spDataFrame\n",
    "\n",
    "\n",
    "from pyspark.ml import Pipeline, PipelineModel\n",
    "from pyspark.ml.feature import (Binarizer, OneHotEncoder, StringIndexer, HashingTF, \n",
    "                                Tokenizer, StandardScaler, VectorAssembler,\n",
    "                               OneHotEncoder, StringIndexer, VectorIndexer)\n",
    "from pyspark.ml.classification import LogisticRegression, LinearSVC, LinearSVCModel\n",
    "from pyspark.ml.evaluation import HasFeaturesCol\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "#from tokenizer import Tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is to setup the sparksession. If additional resources need to be allocated it is done with this command"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "spark = SparkSession \\\n",
    "        .builder \\\n",
    "        .appName('Capstone Analysis') \\\n",
    "        .config(\"spark.driver.memory\", \"60g\") \\\n",
    "        .config(\"spark.executor.memory\", \"60g\") \\\n",
    "        .config(\"spark.executor.cores\", \"5\") \\\n",
    "        .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.3.2'"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "proc_data_schema = [StructField('time',IntegerType(),True),\n",
    "                   StructField('user@domain',StringType(),True),\n",
    "                   StructField('src_comp',StringType(),True),\n",
    "                   StructField('proc_name',StringType(),True),\n",
    "                   StructField('strt',StringType())]\n",
    "proc_final_struc = StructType(fields = proc_data_schema)\n",
    "proc = spark.read.csv('../Dataset/proc.txt',schema=proc_final_struc)\n",
    "\n",
    "auth_data_schema = [StructField('time',IntegerType(),True),\n",
    "                   StructField('src_user@domain',StringType(),True),\n",
    "                   StructField('dest_user@domain',StringType(),True),\n",
    "                   StructField('src_comp',StringType(),True),\n",
    "                   StructField('dest_comp',StringType(),True),\n",
    "                   StructField('auth_type',StringType(),True),\n",
    "                   StructField('logon_type',StringType(),True),\n",
    "                   StructField('auth_orient',StringType(),True),\n",
    "                   StructField('success',StringType(),True)]\n",
    "auth_final_struc = StructType(fields = auth_data_schema)\n",
    "auth = spark.read.csv('../Dataset/auth.txt',schema=auth_final_struc )\n",
    "\n",
    "flows_data_schema = [StructField('time',IntegerType(),False),\n",
    "                   StructField('dur',IntegerType(),False),\n",
    "                   StructField('src_comp',StringType(),False),\n",
    "                   StructField('src_port',StringType(),False),\n",
    "                   StructField('dest_comp',StringType(),False),\n",
    "                   StructField('dest_port',StringType(),False),\n",
    "                   StructField('protocol',StringType(),False),\n",
    "                   StructField('pkt_cnt',IntegerType(),False),\n",
    "                   StructField('byte_cnt',IntegerType(),False)]\n",
    "flows_final_struc = StructType(fields = flows_data_schema)\n",
    "flows = spark.read.csv('../Dataset/flows.txt',schema=flows_final_struc )\n",
    "\n",
    "dns_data_schema = [StructField('time',IntegerType(),True),\n",
    "                   StructField('src_comp',StringType(),True),\n",
    "                   StructField('cmp_resolved',StringType(),True)]\n",
    "dns_final_struc = StructType(fields = dns_data_schema)\n",
    "dns = spark.read.csv('../Dataset/dns.txt',schema=dns_final_struc)\n",
    "\n",
    "redteam_data_schema = [StructField('time',IntegerType(),True),\n",
    "                   StructField('user@domain',StringType(),True),\n",
    "                   StructField('src_comp',StringType(),True),\n",
    "                   StructField('dest_comp',StringType(),True)]\n",
    "redteam_final_struc = StructType(fields = redteam_data_schema)\n",
    "redteam = spark.read.csv('../Dataset/redteam.txt',schema=redteam_final_struc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code is supposed to map the DNS connections into a network map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dns_graph = new_df.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new_dns_graph = dns_graph.sample(5000)\n",
    "# G = nx.from_pandas_edgelist(new_dns_graph, 'src_comp','cmp_resolved','count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from operator import itemgetter\n",
    "# node_and_degree = G.degree()\n",
    "# (largest_hub, degree) = sorted(node_and_degree, key=itemgetter(1))[-1]\n",
    "# hub_ego = nx.ego_graph(G, largest_hub)\n",
    "# # Draw graph\n",
    "# pos = nx.spring_layout(hub_ego)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nx.draw(G,pos,node_color='b', node_size=50, with_labels=False)\n",
    "# nx.draw_networkx_nodes(hub_ego, pos, nodelist=[largest_hub], node_size=300, node_color='r')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transform Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will be where we split all user@domain columns into user and domain columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "proc_split = F.split(proc['user@domain'],'@')\n",
    "proc = proc.withColumn('src_user',proc_split.getItem(0))\n",
    "proc = proc.withColumn('src_dmn',proc_split.getItem(1))\n",
    "proc = proc.drop('user@domain')\n",
    "\n",
    "proc = proc.withColumn('type',F.lit('Process'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "auth_src_split = F.split(auth['src_user@domain'],'@')\n",
    "auth = auth.withColumn('src_user',auth_src_split.getItem(0))\n",
    "auth = auth.withColumn('src_dmn',auth_src_split.getItem(1))\n",
    "\n",
    "auth_dest_split = F.split(auth['dest_user@domain'],'@')\n",
    "auth = auth.withColumn('dest_user',auth_dest_split.getItem(0))\n",
    "auth = auth.withColumn('dest_dmn',auth_dest_split.getItem(1))\n",
    "\n",
    "auth = auth.drop('src_user@domain','dest_user@domain')\n",
    "\n",
    "auth = auth.withColumn('type',F.lit('Auth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "redteam_split = F.split(redteam['user@domain'],'@')\n",
    "redteam = redteam.withColumn('src_user',redteam_split.getItem(0))\n",
    "redteam = redteam.withColumn('src_dmn',redteam_split.getItem(1))\n",
    "\n",
    "redteam = redteam.drop('user@domain')\n",
    "\n",
    "redteam = redteam.withColumn('type',F.lit('RedTeam'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "flows = flows.withColumn('avg_pkt_size', (flows['byte_cnt']/flows['pkt_cnt']).cast(DoubleType()))\n",
    "flows = flows.na.drop(how='all')\n",
    "\n",
    "flows = flows.withColumn('type',F.lit('DataFlow'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test/Train split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_days = 29\n",
    "split_range = split_days * 3600 * 24"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split data into train/test segments. This will be done on the first number of days\n",
    "\n",
    "redteam = redteam.filter(redteam.time <= split_range)\n",
    "\n",
    "dns = dns.filter(dns.time <= split_range)\n",
    "\n",
    "proc = proc.filter(proc.time <= split_range)\n",
    "\n",
    "flows = flows.filter(flows.time <= split_range)\n",
    "\n",
    "auth = auth.filter(auth.time <= split_range)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# days = 20\n",
    "# train_range = days * 3600 * 24"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Split data into train/test segments. This will be done on the first number of days\n",
    "\n",
    "# redteam_test = redteam.filter(redteam.time > train_range)\n",
    "# redteam_train = redteam.filter(redteam.time <= train_range)\n",
    "\n",
    "# dns_test = dns.filter(dns.time > train_range)\n",
    "# dns_train = dns.filter(dns.time <= train_range)\n",
    "\n",
    "# proc_test = proc.filter(proc.time > train_range)\n",
    "# proc_train = proc.filter(proc.time <= train_range)\n",
    "\n",
    "# flows_test = flows.filter(flows.time > train_range)\n",
    "# flows_train = flows.filter(flows.time <= train_range)\n",
    "\n",
    "# auth_test = auth.filter(auth.time > train_range)\n",
    "# auth_train = auth.filter(auth.time <= train_range)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# colum = proc.columns\n",
    "# colum.sort()\n",
    "\n",
    "# proc = proc.select(colum)\n",
    "# redteam = redteam.select(colum)\n",
    "# auth = auth.select(colum)\n",
    "# flows = flows.select(colum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# master = unionAll(redteam,auth,proc,flows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(master.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# master.select('strt').sort('strt').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code was supposed to transform the time column from an int into a datetime data type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# redteam1 = redteam.select(F.to_date(redteam.time,'MM-dd HH:mm:ss').alias('date')).collect()\n",
    "#redteam1 = redteam.rdd.map(lambda x: (x['time'], time.strftime('%m/%d %H:%M:%S', time.gmtime(x['time']) ))).toDF(['time','timestam'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DNS Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dns_train.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# F.pandas_udf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dns_extract(dns_dataset):\n",
    "    dns_count = dns_dataset.groupby(dns_dataset.time,dns_dataset.src_comp).count().na.fill(0).sort('time')\n",
    "    dns_count = dns_count.select(F.col('time'),F.col('src_comp'),F.col('count').alias('dns_count'))\n",
    "    \n",
    "    final_dns_dataset = dns_count\n",
    "    \n",
    "    temp_dns = first_dns_extract(dns_dataset)\n",
    "    \n",
    "    final_dns_dataset = final_dns_dataset.join(temp_dns,['time','src_comp'],'left')\n",
    "    \n",
    "    final_dns_dataset = final_dns_dataset.na.fill(0)\n",
    "    \n",
    "    final_dns_dataset = final_dns_dataset.sort(['time','src_comp'])\n",
    "    \n",
    "    return final_dns_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def first_dns_extract(dns_dataset):\n",
    "    newresolved = dns_dataset.groupby(dns_dataset.src_comp,dns_dataset.cmp_resolved).agg(F.min('time').alias('time')).sort('time')\n",
    "    newresolved = newresolved.repartition(10).groupby(newresolved.time,newresolved.src_comp).count()\n",
    "    newresolved = newresolved.repartition(10).select(F.col('time'),F.col('src_comp'),F.col('count').alias('newresolved_count')).sort('time')\n",
    "    \n",
    "    first_dns_dataset = newresolved\n",
    "    return first_dns_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "dns_sample = dns_extract(dns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 30.502097606658936 seconds ---\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "dns_sample.coalesce(1).write.csv('../Dataset/Output/total/dns_output_1.csv')\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start_time = time.time()\n",
    "\n",
    "# dns_sample.count()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Proc Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# proc_train.select('strt').distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# proc_subset = proc_train.filter((proc_train.time<2)&(proc_train.time>=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# proc_subset.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "#proc_subset_test = proc_subset.groupby(proc_subset.time,proc_subset.src_comp,proc_subset.proc_name,proc_subset.strt).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# proc_subset_test.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# proc_subset_test = proc_subset.groupby(proc_subset.time,proc_subset.src_comp)\\\n",
    "#                     .agg(F.sum(F.when(proc_subset.strt=='Start',F.lit(1)).otherwise(F.lit(-1))).alias('LoggedOn'))\\\n",
    "#                     .withColumn('LoggedOn',F.greatest(F.lit(0),'LoggedOn'))\\\n",
    "#                     .sort('time','src_comp')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# proc_subset_test.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# proc_count = proc_train.groupby(proc_train.time,proc_train.src_comp).count().na.fill(0).sort('time')\n",
    "# proc_count = proc_count.select(F.col('time'),F.col('src_comp'),F.col('count').alias('proc_total'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "# proc_count.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "# proc_count.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "# proc_exec = proc_train.filter(proc_train.strt=='Start').groupby(proc_train.time,proc_train.src_comp).count().na.fill(0).sort('time')\n",
    "# proc_exec = proc_exec.select(F.col('time'),F.col('src_comp'),F.col('count').alias('proc_exec_total'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "# proc_exec.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "# proc_exec.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "def first_proc_extract(proc_dataset):\n",
    "    newexecute = proc_dataset.filter(proc_dataset.strt=='Start').groupby(proc_dataset.src_comp,proc_dataset.proc_name).agg(F.min('time').alias('time')).sort('time')\n",
    "    newexecute = newexecute.groupby(newexecute.time,newexecute.src_comp).count()\n",
    "    newexecute = newexecute.select(F.col('time'),F.col('src_comp'),F.col('count').alias('newexecute_count')).sort('time')\n",
    "    \n",
    "    first_execute_dataset = newexecute\n",
    "    return first_execute_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "def proc_running_extract(proc_dataset):\n",
    "    proc_running = proc_dataset.groupby(proc_dataset.time,proc_dataset.src_comp)\\\n",
    "                    .agg(F.sum(F.when(proc_dataset.strt=='Start',F.lit(1)).otherwise(F.lit(-1))).alias('Proc_run'))\\\n",
    "                    .withColumn('Proc_run',F.greatest(F.lit(0),'Proc_run'))\\\n",
    "                    .sort('time','src_comp')\n",
    "    return proc_running\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "def proc_extract(proc_dataset):\n",
    "    proc_count = proc_dataset.groupby(proc_dataset.time,proc_dataset.src_comp).count().na.fill(0).sort('time')\n",
    "    proc_count = proc_count.select(F.col('time'),F.col('src_comp'),F.col('count').alias('proc_total'))\n",
    "    \n",
    "    proc_exec = proc_dataset.filter(proc_dataset.strt=='Start').groupby(proc_dataset.time,proc_dataset.src_comp).count().na.fill(0).sort('time')\n",
    "    proc_exec = proc_exec.select(F.col('time'),F.col('src_comp'),F.col('count').alias('proc_exec_total'))\n",
    "    \n",
    "    proc_running = proc_dataset.groupby(proc_dataset.time,proc_dataset.src_comp)\\\n",
    "                    .agg(F.sum(F.when(proc_dataset.strt=='Start',F.lit(1)).otherwise(F.lit(-1))).alias('Proc_run'))\\\n",
    "                    .withColumn('Proc_run',F.greatest(F.lit(0),'Proc_run'))\\\n",
    "                    .sort('time','src_comp')\n",
    "    \n",
    "    first_execute = first_proc_extract(proc_dataset)\n",
    "    \n",
    "    \n",
    "    final_proc_dataset=proc_count\n",
    "    \n",
    "    final_proc_dataset = final_proc_dataset.join(proc_exec,['time','src_comp'],'left')\n",
    "    \n",
    "    final_proc_dataset = final_proc_dataset.join(first_execute,['time','src_comp'],'left')\n",
    "    \n",
    "    final_proc_dataset = final_proc_dataset.join(proc_running,['time','src_comp'],'left')\n",
    "    \n",
    "    final_proc_dataset = final_proc_dataset.na.fill(0)\n",
    "    \n",
    "    final_proc_dataset = final_proc_dataset.sort(['time','src_comp'])\n",
    "    \n",
    "    \n",
    "    return final_proc_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "proc_sample = proc_extract(proc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 415.9814863204956 seconds ---\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "proc_sample.coalesce(1).write.csv('../Dataset/Output/total/proc_output_1')\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Auth Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "def auth_extract(auth_dataset):\n",
    "    failed_logon = auth_dataset.filter(auth_dataset.success=='Fail').groupby(auth_dataset.time,auth_dataset.src_comp).count().na.fill(0).sort('time')\n",
    "    failed_logon = failed_logon.select(F.col('time'),F.col('src_comp'),F.col('count').alias('fail_count'))\n",
    "    \n",
    "    \n",
    "    final_auth_dataset = failed_logon\n",
    "    return final_auth_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "def auth_loggedon_extract(auth_dataset):\n",
    "    auth_loggedon = auth_dataset.filter((auth_dataset.success==\"Success\") & ((auth_dataset.auth_orient=='LogOn')|(auth_dataset.auth_orient=='LogOff'))).groupby(auth_dataset.time,auth_dataset.src_comp)\\\n",
    "                    .agg(F.sum(F.when(auth_dataset.auth_orient=='LogOn',F.lit(1)).otherwise(F.lit(-1))).alias('LoggedOn'))\\\n",
    "                    .withColumn('LoggedOn',F.greatest(F.lit(0),'LoggedOn'))\\\n",
    "                    .sort('time','src_comp')\n",
    "    return auth_loggedon\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "def joined_auth_extract(auth_dataset):\n",
    "\n",
    "    failed_logon = auth_dataset.filter(auth_dataset.success=='Fail').groupby(auth_dataset.time,auth_dataset.src_comp).count().na.fill(0).sort('time')\n",
    "    failed_logon = failed_logon.select(F.col('time'),F.col('src_comp'),F.col('count').alias('fail_count'))\n",
    "    \n",
    "    auth_loggedon = auth_loggedon_extract(auth_dataset)\n",
    "    \n",
    "    final_auth_dataset = failed_logon.join(auth_loggedon,['time','src_comp'],'outer' )\n",
    "    return final_auth_dataset\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "auth_sample = joined_auth_extract(auth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 568.9997189044952 seconds ---\n"
     ]
    }
   ],
   "source": [
    "auth_sample.coalesce(1).write.csv('../Dataset/Output/total/auth_output_1.csv')\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Redteam Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "def redteam_extract(redteam_dataset):\n",
    "    redteam_event = redteam_dataset.groupby(redteam_dataset.time, redteam_dataset.src_comp).count().na.fill(0).sort('time')\n",
    "    redteam_event = redteam_event.select(F.col('time'),F.col('src_comp'),F.col('count').alias('redteam_event'))\n",
    "    \n",
    "    final_redteam_dataset = redteam_event\n",
    "    return final_redteam_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 1.0521724224090576 seconds ---\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "redteam_sample = redteam_extract(redteam)\n",
    "redteam_sample.coalesce(1).write.csv('../Dataset/Output/total/redteam_output_1.csv')\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Flows Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# flows_train.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# flows_data = flows_train.select('time','dur','src_comp','pkt_cnt','byte_cnt','avg_pkt_size')\\\n",
    "#                         .groupby('time','src_comp').agg(F.sum('dur').alias('dur'),F.sum('pkt_cnt').alias('pkt_cnt'),F.sum('byte_cnt').alias('byte_cnt'),F.avg('avg_pkt_size').alias('avg_pkt_size')).sort('time')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# flows_data.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flows_extract(flows_dataset):\n",
    "    flows_data = flows_dataset.select('time','dur','src_comp','pkt_cnt','byte_cnt','avg_pkt_size')\\\n",
    "                        .groupby('time','src_comp').agg(F.sum('dur').alias('dur'),F.sum('pkt_cnt').alias('pkt_cnt'),F.sum('byte_cnt').alias('byte_cnt'),F.avg('avg_pkt_size').alias('avg_pkt_size')).sort('time')\n",
    "    \n",
    "    final_flows_dataset = flows_data\n",
    "    return final_flows_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "flows_sample = flows_extract(flows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "# flows_sample.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 109.16060328483582 seconds ---\n"
     ]
    }
   ],
   "source": [
    "flows_sample.coalesce(1).write.csv('../Dataset/Output/total/flows_output_1.csv')\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combine Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "redteam_columns = ['time', 'src_comp', 'redteam_event']\n",
    "auth_columns = ['time', 'src_comp', 'fail_count', 'LoggedOn']\n",
    "dns_columns = ['time', 'src_comp', 'dns_count', 'newresolved_count']\n",
    "proc_columns = ['time', 'src_comp', 'proc_total', 'proc_exec_total', 'newexecute_count', 'Proc_run']\n",
    "flows_columns = ['time', 'src_comp', 'dur', 'pkt_cnt', 'byte_cnt', 'avg_pkt_size']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imported  ../Dataset/Output/total/dns_output_1.csv/part-00000-68cf72dc-3603-4e3f-b4ce-021e850f3851-c000.csv\n",
      "imported  ../Dataset/Output/total/proc_output_1/part-00000-f223434c-e14b-4b8c-894c-a6c838c645c4-c000.csv\n",
      "imported  ../Dataset/Output/total/auth_output_1.csv/part-00000-d7179e49-f4c3-4a47-9578-458f39d524af-c000.csv\n",
      "imported  ../Dataset/Output/total/flows_output_1.csv/part-00000-02a3a881-6d3a-4135-b118-e60fee765170-c000.csv\n",
      "imported  ../Dataset/Output/total/redteam_output_1.csv/part-00000-3fe1cf7e-a374-4c41-9b7d-d378c4c40714-c000.csv\n"
     ]
    }
   ],
   "source": [
    "directory = \"../Dataset/Output/total/\"\n",
    "for root, dirs, files in os.walk(directory):\n",
    "    for file in files:\n",
    "        if file.endswith('.csv'):\n",
    "            filepath = root+'/'+file\n",
    "            if \"dns\" in root:   \n",
    "                pd_dns_df = pd.read_csv(filepath, names=dns_columns)\n",
    "                print(\"imported \", filepath)\n",
    "            elif \"proc\" in root:\n",
    "                pd_proc_df = pd.read_csv(filepath, names=proc_columns)\n",
    "                print(\"imported \", filepath)\n",
    "            elif \"flows\" in root:   \n",
    "                pd_flows_df = pd.read_csv(filepath, names=flows_columns)\n",
    "                print(\"imported \", filepath)\n",
    "            elif \"redteam\" in root:\n",
    "                pd_redteam_df = pd.read_csv(filepath, names=redteam_columns)\n",
    "                print(\"imported \", filepath)\n",
    "            elif \"auth\" in root:\n",
    "                pd_auth_df = pd.read_csv(filepath, names=auth_columns)\n",
    "                print(\"imported \", filepath)\n",
    "            else:\n",
    "                print(\"Error importing \", file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done with round  1  of  15\n",
      "Done with round  2  of  15\n",
      "Done with round  3  of  15\n",
      "Done with round  4  of  15\n",
      "Done with round  5  of  15\n",
      "Done with round  6  of  15\n",
      "Done with round  7  of  15\n",
      "Done with round  8  of  15\n",
      "Done with round  9  of  15\n",
      "Done with round  10  of  15\n",
      "Done with round  11  of  15\n",
      "Done with round  12  of  15\n",
      "Done with round  13  of  15\n",
      "Done with round  14  of  15\n",
      "Done with round  15  of  15\n"
     ]
    }
   ],
   "source": [
    "file = '../Dataset/Output/complete_df.csv'\n",
    "\n",
    "n_size = pd_auth_df.time.max()\n",
    "\n",
    "splits = 15\n",
    "\n",
    "for n in range(int(splits)):\n",
    "\n",
    "    lower = int(n_size/splits*n)\n",
    "    upper = int(n_size/splits*(n+1))\n",
    "    \n",
    "    redteam = pd_redteam_df[(pd_redteam_df.time > lower) & (pd_redteam_df.time <= upper)]\n",
    "\n",
    "    dns = pd_dns_df[(pd_dns_df.time > lower) & (pd_dns_df.time <= upper)]\n",
    "\n",
    "    proc = pd_proc_df[(pd_proc_df.time > lower) & (pd_proc_df.time <= upper)]\n",
    "\n",
    "    flows = pd_flows_df[(pd_flows_df.time > lower) & (pd_flows_df.time <= upper)]\n",
    "\n",
    "    auth = pd_auth_df[(pd_auth_df.time > lower) & (pd_auth_df.time <= upper)]\n",
    "\n",
    "    master_df = dns.merge(auth,on=['time','src_comp'],how='outer')\n",
    "    master_df = master_df.merge(flows,on=['time','src_comp'],how='outer')\n",
    "    master_df = master_df.merge(redteam,on=['time','src_comp'],how='outer')\n",
    "    master_df = master_df.merge(proc,on=['time','src_comp'],how='outer')\n",
    "\n",
    "    master_df=master_df.fillna(-999)\n",
    "    \n",
    "#     master_df['time'] = pd.to_datetime(master_df['time'],unit='s').dt.strftime('%m/%d %H:%M:%S').head()\n",
    "\n",
    "    if not os.path.isfile(file):\n",
    "        master_df.to_csv(file, index=False)\n",
    "    else:\n",
    "        master_df.to_csv(file,mode='a',header=False, index=False )\n",
    "    print('Done with round ', (n+1),' of ',(splits), flush=True)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This block will save a csv with all data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = '../Dataset/Output/complete_5m_df.csv'\n",
    "\n",
    "splits = 15\n",
    "\n",
    "pd_dns_df['time'] = pd.to_datetime(pd_dns_df['time'], unit='s').apply(lambda dt: dt.replace(year=2017))\n",
    "pd_flows_df['time'] = pd.to_datetime(pd_flows_df['time'], unit='s').apply(lambda dt: dt.replace(year=2017))\n",
    "pd_auth_df['time'] = pd.to_datetime(pd_auth_df['time'], unit='s').apply(lambda dt: dt.replace(year=2017))\n",
    "pd_redteam_df['time'] = pd.to_datetime(pd_redteam_df['time'], unit='s').apply(lambda dt: dt.replace(year=2017))\n",
    "pd_proc_df['time'] = pd.to_datetime(pd_proc_df['time'], unit='s').apply(lambda dt: dt.replace(year=2017))\n",
    "\n",
    "resample_pd_dns_df = pd_dns_df.set_index('time').groupby('src_comp').resample('5min').agg({\n",
    "                                                                       'dns_count':'sum',\n",
    "                                                                       'newresolved_count':'sum'} ).reset_index().fillna(-999)\n",
    "resample_pd_flows_df = pd_flows_df.set_index('time').groupby('src_comp').resample('5min').agg({\n",
    "                                                                       'dur':'sum',\n",
    "                                                                       'pkt_cnt':'sum',\n",
    "                                                                       'byte_cnt':'sum',\n",
    "                                                                       'avg_pkt_size':'mean'} ).reset_index().fillna(-999)\n",
    "resample_pd_auth_df = pd_auth_df.set_index('time').groupby('src_comp').resample('5min').agg({\n",
    "                                                                       'fail_count':'sum',\n",
    "                                                                       'LoggedOn':'sum'} ).reset_index().fillna(-999)\n",
    "resample_pd_redteam_df = pd_redteam_df.set_index('time').groupby('src_comp').resample('5min').agg({'redteam_event':'sum'\n",
    "                                                                    } ).reset_index().fillna(-999)\n",
    "resample_pd_proc_df = pd_proc_df.set_index('time').groupby('src_comp').resample('5min').agg({\n",
    "                                                                       'proc_total':'sum',\n",
    "                                                                       'proc_exec_total':'sum',\n",
    "                                                                       'newexecute_count':'sum',\n",
    "                                                                       'Proc_run':'sum'} ).reset_index().fillna(-999)\n",
    "\n",
    "master_5m_df = resample_pd_auth_df.merge(resample_pd_dns_df,on=['time','src_comp'],how='outer')\n",
    "master_5m_df = master_5m_df.merge(resample_pd_flows_df,on=['time','src_comp'],how='outer')\n",
    "master_5m_df = master_5m_df.merge(resample_pd_redteam_df,on=['time','src_comp'],how='outer')\n",
    "master_5m_df = master_5m_df.merge(resample_pd_proc_df,on=['time','src_comp'],how='outer')\n",
    "\n",
    "master_5m_df=master_5m_df.fillna(-999)\n",
    "\n",
    "master_5m_df.to_csv(file, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stop running up to here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Classification model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = StratifiedKFold(n_splits=6)\n",
    "classifier = xgb.XGBClassifier()\n",
    "\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n",
    "\n",
    "\n",
    "tprs = []\n",
    "aucs = []\n",
    "mean_fpr = np.linspace(0, 1, 100)\n",
    "\n",
    "i = 0\n",
    "for train, test in cv.split(X, y):\n",
    "    print('round ',i+1)\n",
    "    probas_ = classifier.fit(X[train], y[train]).predict_proba(X[test])\n",
    "    # Compute ROC curve and area the curve\n",
    "    fpr, tpr, thresholds = roc_curve(y[test], probas_[:, 1])\n",
    "    tprs.append(interp(mean_fpr, fpr, tpr))\n",
    "    tprs[-1][0] = 0.0\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    aucs.append(roc_auc)\n",
    "    plt.plot(fpr, tpr, lw=1, alpha=0.3,\n",
    "             label='ROC fold %d (AUC = %0.2f)' % (i, roc_auc))\n",
    "\n",
    "    i += 1\n",
    "plt.plot([0, 1], [0, 1], linestyle='--', lw=2, color='r',\n",
    "         label='Chance', alpha=.8)\n",
    "\n",
    "mean_tpr = np.mean(tprs, axis=0)\n",
    "mean_tpr[-1] = 1.0\n",
    "mean_auc = auc(mean_fpr, mean_tpr)\n",
    "std_auc = np.std(aucs)\n",
    "plt.plot(mean_fpr, mean_tpr, color='b',\n",
    "         label=r'Mean ROC (AUC = %0.2f $\\pm$ %0.2f)' % (mean_auc, std_auc),\n",
    "         lw=2, alpha=.8)\n",
    "\n",
    "std_tpr = np.std(tprs, axis=0)\n",
    "tprs_upper = np.minimum(mean_tpr + std_tpr, 1)\n",
    "tprs_lower = np.maximum(mean_tpr - std_tpr, 0)\n",
    "plt.fill_between(mean_fpr, tprs_lower, tprs_upper, color='grey', alpha=.2,\n",
    "                 label=r'$\\pm$ 1 std. dev.')\n",
    "\n",
    "plt.xlim([-0.05, 1.05])\n",
    "plt.ylim([-0.05, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver operating characteristic example')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
