{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime, time\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "from functools import reduce\n",
    "from pyspark.sql import Window\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import (StructField, StringType,FloatType, \n",
    "                               DoubleType, IntegerType, StructType,\n",
    "                              DateType)\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import DataFrame as spDataFrame\n",
    "\n",
    "\n",
    "from pyspark.ml import Pipeline, PipelineModel\n",
    "from pyspark.ml.feature import (Binarizer, OneHotEncoder, StringIndexer, HashingTF, \n",
    "                                Tokenizer, StandardScaler, VectorAssembler,\n",
    "                               OneHotEncoder, StringIndexer, VectorIndexer)\n",
    "from pyspark.ml.classification import LogisticRegression, LinearSVC, LinearSVCModel\n",
    "from pyspark.ml.evaluation import HasFeaturesCol\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "#from tokenizer import Tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is to setup the sparksession. If additional resources need to be allocated it is done with this command"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "spark = SparkSession \\\n",
    "        .builder \\\n",
    "        .appName('Capstone Analysis') \\\n",
    "        .config(\"spark.driver.memory\", \"60g\") \\\n",
    "        .config(\"spark.executor.memory\", \"60g\") \\\n",
    "        .config(\"spark.executor.cores\", \"5\") \\\n",
    "        .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.3.2'"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "proc_data_schema = [StructField('time',IntegerType(),True),\n",
    "                   StructField('user@domain',StringType(),True),\n",
    "                   StructField('src_comp',StringType(),True),\n",
    "                   StructField('proc_name',StringType(),True),\n",
    "                   StructField('strt',StringType())]\n",
    "proc_final_struc = StructType(fields = proc_data_schema)\n",
    "proc = spark.read.csv('../Dataset/proc.txt',schema=proc_final_struc)\n",
    "\n",
    "auth_data_schema = [StructField('time',IntegerType(),True),\n",
    "                   StructField('src_user@domain',StringType(),True),\n",
    "                   StructField('dest_user@domain',StringType(),True),\n",
    "                   StructField('src_comp',StringType(),True),\n",
    "                   StructField('dest_comp',StringType(),True),\n",
    "                   StructField('auth_type',StringType(),True),\n",
    "                   StructField('logon_type',StringType(),True),\n",
    "                   StructField('auth_orient',StringType(),True),\n",
    "                   StructField('success',StringType(),True)]\n",
    "auth_final_struc = StructType(fields = auth_data_schema)\n",
    "auth = spark.read.csv('../Dataset/auth.txt',schema=auth_final_struc )\n",
    "\n",
    "flows_data_schema = [StructField('time',IntegerType(),False),\n",
    "                   StructField('dur',IntegerType(),False),\n",
    "                   StructField('src_comp',StringType(),False),\n",
    "                   StructField('src_port',StringType(),False),\n",
    "                   StructField('dest_comp',StringType(),False),\n",
    "                   StructField('dest_port',StringType(),False),\n",
    "                   StructField('protocol',StringType(),False),\n",
    "                   StructField('pkt_cnt',IntegerType(),False),\n",
    "                   StructField('byte_cnt',IntegerType(),False)]\n",
    "flows_final_struc = StructType(fields = flows_data_schema)\n",
    "flows = spark.read.csv('../Dataset/flows.txt',schema=flows_final_struc )\n",
    "\n",
    "dns_data_schema = [StructField('time',IntegerType(),True),\n",
    "                   StructField('src_comp',StringType(),True),\n",
    "                   StructField('cmp_resolved',StringType(),True)]\n",
    "dns_final_struc = StructType(fields = dns_data_schema)\n",
    "dns = spark.read.csv('../Dataset/dns.txt',schema=dns_final_struc)\n",
    "\n",
    "redteam_data_schema = [StructField('time',IntegerType(),True),\n",
    "                   StructField('user@domain',StringType(),True),\n",
    "                   StructField('src_comp',StringType(),True),\n",
    "                   StructField('dest_comp',StringType(),True)]\n",
    "redteam_final_struc = StructType(fields = redteam_data_schema)\n",
    "redteam = spark.read.csv('../Dataset/redteam.txt',schema=redteam_final_struc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code is supposed to map the DNS connections into a network map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dns_graph = new_df.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new_dns_graph = dns_graph.sample(5000)\n",
    "# G = nx.from_pandas_edgelist(new_dns_graph, 'src_comp','cmp_resolved','count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from operator import itemgetter\n",
    "# node_and_degree = G.degree()\n",
    "# (largest_hub, degree) = sorted(node_and_degree, key=itemgetter(1))[-1]\n",
    "# hub_ego = nx.ego_graph(G, largest_hub)\n",
    "# # Draw graph\n",
    "# pos = nx.spring_layout(hub_ego)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nx.draw(G,pos,node_color='b', node_size=50, with_labels=False)\n",
    "# nx.draw_networkx_nodes(hub_ego, pos, nodelist=[largest_hub], node_size=300, node_color='r')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transform Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will be where we split all user@domain columns into user and domain columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "proc_split = F.split(proc['user@domain'],'@')\n",
    "proc = proc.withColumn('src_user',proc_split.getItem(0))\n",
    "proc = proc.withColumn('src_dmn',proc_split.getItem(1))\n",
    "proc = proc.drop('user@domain')\n",
    "\n",
    "proc = proc.withColumn('type',F.lit('Process'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "auth_src_split = F.split(auth['src_user@domain'],'@')\n",
    "auth = auth.withColumn('src_user',auth_src_split.getItem(0))\n",
    "auth = auth.withColumn('src_dmn',auth_src_split.getItem(1))\n",
    "\n",
    "auth_dest_split = F.split(auth['dest_user@domain'],'@')\n",
    "auth = auth.withColumn('dest_user',auth_dest_split.getItem(0))\n",
    "auth = auth.withColumn('dest_dmn',auth_dest_split.getItem(1))\n",
    "\n",
    "auth = auth.drop('src_user@domain','dest_user@domain')\n",
    "\n",
    "auth = auth.withColumn('type',F.lit('Auth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "redteam_split = F.split(redteam['user@domain'],'@')\n",
    "redteam = redteam.withColumn('src_user',redteam_split.getItem(0))\n",
    "redteam = redteam.withColumn('src_dmn',redteam_split.getItem(1))\n",
    "\n",
    "redteam = redteam.drop('user@domain')\n",
    "\n",
    "redteam = redteam.withColumn('type',F.lit('RedTeam'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "flows = flows.withColumn('avg_pkt_size', (flows['byte_cnt']/flows['pkt_cnt']).cast(DoubleType()))\n",
    "flows = flows.na.drop(how='all')\n",
    "\n",
    "flows = flows.withColumn('type',F.lit('DataFlow'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test/Train split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_days = 29\n",
    "split_range = split_days * 3600 * 24"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split data into train/test segments. This will be done on the first number of days\n",
    "\n",
    "redteam = redteam.filter(redteam.time <= split_range)\n",
    "\n",
    "dns = dns.filter(dns.time <= split_range)\n",
    "\n",
    "proc = proc.filter(proc.time <= split_range)\n",
    "\n",
    "flows = flows.filter(flows.time <= split_range)\n",
    "\n",
    "auth = auth.filter(auth.time <= split_range)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# days = 20\n",
    "# train_range = days * 3600 * 24"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Split data into train/test segments. This will be done on the first number of days\n",
    "\n",
    "# redteam_test = redteam.filter(redteam.time > train_range)\n",
    "# redteam_train = redteam.filter(redteam.time <= train_range)\n",
    "\n",
    "# dns_test = dns.filter(dns.time > train_range)\n",
    "# dns_train = dns.filter(dns.time <= train_range)\n",
    "\n",
    "# proc_test = proc.filter(proc.time > train_range)\n",
    "# proc_train = proc.filter(proc.time <= train_range)\n",
    "\n",
    "# flows_test = flows.filter(flows.time > train_range)\n",
    "# flows_train = flows.filter(flows.time <= train_range)\n",
    "\n",
    "# auth_test = auth.filter(auth.time > train_range)\n",
    "# auth_train = auth.filter(auth.time <= train_range)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# colum = proc.columns\n",
    "# colum.sort()\n",
    "\n",
    "# proc = proc.select(colum)\n",
    "# redteam = redteam.select(colum)\n",
    "# auth = auth.select(colum)\n",
    "# flows = flows.select(colum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# master = unionAll(redteam,auth,proc,flows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(master.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# master.select('strt').sort('strt').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code was supposed to transform the time column from an int into a datetime data type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# redteam1 = redteam.select(F.to_date(redteam.time,'MM-dd HH:mm:ss').alias('date')).collect()\n",
    "#redteam1 = redteam.rdd.map(lambda x: (x['time'], time.strftime('%m/%d %H:%M:%S', time.gmtime(x['time']) ))).toDF(['time','timestam'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DNS Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dns_train.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# F.pandas_udf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dns_extract(dns_dataset):\n",
    "    dns_count = dns_dataset.groupby(dns_dataset.time,dns_dataset.src_comp).count().na.fill(0).sort('time')\n",
    "    dns_count = dns_count.select(F.col('time'),F.col('src_comp'),F.col('count').alias('dns_count'))\n",
    "    \n",
    "    final_dns_dataset = dns_count\n",
    "    \n",
    "    temp_dns = first_dns_extract(dns_dataset)\n",
    "    \n",
    "    final_dns_dataset = final_dns_dataset.join(temp_dns,['time','src_comp'],'left')\n",
    "    \n",
    "    final_dns_dataset = final_dns_dataset.na.fill(0)\n",
    "    \n",
    "    final_dns_dataset = final_dns_dataset.sort(['time','src_comp'])\n",
    "    \n",
    "    return final_dns_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def first_dns_extract(dns_dataset):\n",
    "    newresolved = dns_dataset.groupby(dns_dataset.src_comp,dns_dataset.cmp_resolved).agg(F.min('time').alias('time')).sort('time')\n",
    "    newresolved = newresolved.repartition(10).groupby(newresolved.time,newresolved.src_comp).count()\n",
    "    newresolved = newresolved.repartition(10).select(F.col('time'),F.col('src_comp'),F.col('count').alias('newresolved_count')).sort('time')\n",
    "    \n",
    "    first_dns_dataset = newresolved\n",
    "    return first_dns_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "dns_sample = dns_extract(dns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 30.502097606658936 seconds ---\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "dns_sample.coalesce(1).write.csv('../Dataset/Output/total/dns_output_1.csv')\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start_time = time.time()\n",
    "\n",
    "# dns_sample.count()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Proc Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# proc_train.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# proc_train.select('strt').distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# proc_subset = proc_train.filter((proc_train.time<2)&(proc_train.time>=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# proc_subset.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "#proc_subset_test = proc_subset.groupby(proc_subset.time,proc_subset.src_comp,proc_subset.proc_name,proc_subset.strt).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# proc_subset_test.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# proc_subset_test = proc_subset.groupby(proc_subset.time,proc_subset.src_comp)\\\n",
    "#                     .agg(F.sum(F.when(proc_subset.strt=='Start',F.lit(1)).otherwise(F.lit(-1))).alias('LoggedOn'))\\\n",
    "#                     .withColumn('LoggedOn',F.greatest(F.lit(0),'LoggedOn'))\\\n",
    "#                     .sort('time','src_comp')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# proc_subset_test.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# proc_count = proc_train.groupby(proc_train.time,proc_train.src_comp).count().na.fill(0).sort('time')\n",
    "# proc_count = proc_count.select(F.col('time'),F.col('src_comp'),F.col('count').alias('proc_total'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "# proc_count.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "# proc_count.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "# proc_exec = proc_train.filter(proc_train.strt=='Start').groupby(proc_train.time,proc_train.src_comp).count().na.fill(0).sort('time')\n",
    "# proc_exec = proc_exec.select(F.col('time'),F.col('src_comp'),F.col('count').alias('proc_exec_total'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "# proc_exec.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "# proc_exec.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "def first_proc_extract(proc_dataset):\n",
    "    newexecute = proc_dataset.filter(proc_dataset.strt=='Start').groupby(proc_dataset.src_comp,proc_dataset.proc_name).agg(F.min('time').alias('time')).sort('time')\n",
    "    newexecute = newexecute.groupby(newexecute.time,newexecute.src_comp).count()\n",
    "    newexecute = newexecute.select(F.col('time'),F.col('src_comp'),F.col('count').alias('newexecute_count')).sort('time')\n",
    "    \n",
    "    first_execute_dataset = newexecute\n",
    "    return first_execute_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "def proc_running_extract(proc_dataset):\n",
    "    proc_running = proc_dataset.groupby(proc_dataset.time,proc_dataset.src_comp)\\\n",
    "                    .agg(F.sum(F.when(proc_dataset.strt=='Start',F.lit(1)).otherwise(F.lit(-1))).alias('Proc_run'))\\\n",
    "                    .withColumn('Proc_run',F.greatest(F.lit(0),'Proc_run'))\\\n",
    "                    .sort('time','src_comp')\n",
    "    return proc_running\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "def proc_extract(proc_dataset):\n",
    "    proc_count = proc_dataset.groupby(proc_dataset.time,proc_dataset.src_comp).count().na.fill(0).sort('time')\n",
    "    proc_count = proc_count.select(F.col('time'),F.col('src_comp'),F.col('count').alias('proc_total'))\n",
    "    \n",
    "    proc_exec = proc_dataset.filter(proc_dataset.strt=='Start').groupby(proc_dataset.time,proc_dataset.src_comp).count().na.fill(0).sort('time')\n",
    "    proc_exec = proc_exec.select(F.col('time'),F.col('src_comp'),F.col('count').alias('proc_exec_total'))\n",
    "    \n",
    "    proc_running = proc_dataset.groupby(proc_dataset.time,proc_dataset.src_comp)\\\n",
    "                    .agg(F.sum(F.when(proc_dataset.strt=='Start',F.lit(1)).otherwise(F.lit(-1))).alias('Proc_run'))\\\n",
    "                    .withColumn('Proc_run',F.greatest(F.lit(0),'Proc_run'))\\\n",
    "                    .sort('time','src_comp')\n",
    "    \n",
    "    first_execute = first_proc_extract(proc_dataset)\n",
    "    \n",
    "    \n",
    "    final_proc_dataset=proc_count\n",
    "    \n",
    "    final_proc_dataset = final_proc_dataset.join(proc_exec,['time','src_comp'],'left')\n",
    "    \n",
    "    final_proc_dataset = final_proc_dataset.join(first_execute,['time','src_comp'],'left')\n",
    "    \n",
    "    final_proc_dataset = final_proc_dataset.join(proc_running,['time','src_comp'],'left')\n",
    "    \n",
    "    final_proc_dataset = final_proc_dataset.na.fill(0)\n",
    "    \n",
    "    final_proc_dataset = final_proc_dataset.sort(['time','src_comp'])\n",
    "    \n",
    "    \n",
    "    return final_proc_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "proc_sample = proc_extract(proc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start_time = time.time()\n",
    "# proc_sample.count()\n",
    "# print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 415.9814863204956 seconds ---\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "proc_sample.coalesce(1).write.csv('../Dataset/Output/total/proc_output_1')\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start_time = time.time()\n",
    "\n",
    "# proc_sample.count()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Auth Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# auth_train.show(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "# failed_logon = auth_train.filter(auth_train.success=='Fail').groupby(auth_train.time,auth_train.src_comp).count().na.fill(0).sort('time')\n",
    "# failed_logon = failed_logon.select(F.col('time'),F.col('src_comp'),F.col('count').alias('fail_count'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "# failed_logon.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "def auth_extract(auth_dataset):\n",
    "    failed_logon = auth_dataset.filter(auth_dataset.success=='Fail').groupby(auth_dataset.time,auth_dataset.src_comp).count().na.fill(0).sort('time')\n",
    "    failed_logon = failed_logon.select(F.col('time'),F.col('src_comp'),F.col('count').alias('fail_count'))\n",
    "    \n",
    "    \n",
    "    final_auth_dataset = failed_logon\n",
    "    return final_auth_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "def auth_loggedon_extract(auth_dataset):\n",
    "    auth_loggedon = auth_dataset.filter((auth_dataset.success==\"Success\") & ((auth_dataset.auth_orient=='LogOn')|(auth_dataset.auth_orient=='LogOff'))).groupby(auth_dataset.time,auth_dataset.src_comp)\\\n",
    "                    .agg(F.sum(F.when(auth_dataset.auth_orient=='LogOn',F.lit(1)).otherwise(F.lit(-1))).alias('LoggedOn'))\\\n",
    "                    .withColumn('LoggedOn',F.greatest(F.lit(0),'LoggedOn'))\\\n",
    "                    .sort('time','src_comp')\n",
    "    return auth_loggedon\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "# auth_sample = auth_loggedon_extract(auth_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start_time = time.time()\n",
    "\n",
    "# auth_sample.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "# auth_sample = auth_extract(auth_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start_time = time.time()\n",
    "\n",
    "# auth_sample.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "def joined_auth_extract(auth_dataset):\n",
    "\n",
    "    failed_logon = auth_dataset.filter(auth_dataset.success=='Fail').groupby(auth_dataset.time,auth_dataset.src_comp).count().na.fill(0).sort('time')\n",
    "    failed_logon = failed_logon.select(F.col('time'),F.col('src_comp'),F.col('count').alias('fail_count'))\n",
    "    \n",
    "    auth_loggedon = auth_loggedon_extract(auth_dataset)\n",
    "    \n",
    "    final_auth_dataset = failed_logon.join(auth_loggedon,['time','src_comp'],'outer' )\n",
    "    return final_auth_dataset\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "# auth_sample = joined_auth_extract(auth_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start_time = time.time()\n",
    "\n",
    "# auth_sample.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "auth_sample = joined_auth_extract(auth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auth_sample.coalesce(1).write.csv('../Dataset/Output/total/auth_output_1.csv')\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# process_start = proc.groupby(proc.time).agg(F.when)\n",
    "# process_start = process_start.a\n",
    "# agg().sort('time')\n",
    "# #process_start = process_start.select(F.col('time'),F.col('strt'),F.col('count').alias('proc_change'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# auth_train.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Redteam Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# redteam_train.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# redteam_event = redteam_train.groupby(redteam_train.time, redteam_train.src_comp).count().na.fill(0).sort('time')\n",
    "# redteam_event = redteam_event.select(F.col('time'),F.col('src_comp'),F.col('count').alias('redteam_event'))\n",
    "# # redteam_event = redteam_event.withColumn('redteam_event',\\\n",
    "# #                                         F.when(redteam_event['redteam_event']>1,2).otherwise(redteam_event['redteam_event']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def redteam_extract(redteam_dataset):\n",
    "    redteam_event = redteam_dataset.groupby(redteam_dataset.time, redteam_dataset.src_comp).count().na.fill(0).sort('time')\n",
    "    redteam_event = redteam_event.select(F.col('time'),F.col('src_comp'),F.col('count').alias('redteam_event'))\n",
    "    \n",
    "    final_redteam_dataset = redteam_event\n",
    "    return final_redteam_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "redteam_sample = redteam_extract(redteam)\n",
    "auth_sample.coalesce(1).write.csv('../Dataset/Output/total/auth_output_1.csv')\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# redteam_event.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Flows Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# flows_train.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# flows_data = flows_train.select('time','dur','src_comp','pkt_cnt','byte_cnt','avg_pkt_size')\\\n",
    "#                         .groupby('time','src_comp').agg(F.sum('dur').alias('dur'),F.sum('pkt_cnt').alias('pkt_cnt'),F.sum('byte_cnt').alias('byte_cnt'),F.avg('avg_pkt_size').alias('avg_pkt_size')).sort('time')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# flows_data.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flows_extract(flows_dataset):\n",
    "    flows_data = flows_dataset.select('time','dur','src_comp','pkt_cnt','byte_cnt','avg_pkt_size')\\\n",
    "                        .groupby('time','src_comp').agg(F.sum('dur').alias('dur'),F.sum('pkt_cnt').alias('pkt_cnt'),F.sum('byte_cnt').alias('byte_cnt'),F.avg('avg_pkt_size').alias('avg_pkt_size')).sort('time')\n",
    "    \n",
    "    final_flows_dataset = flows_data\n",
    "    return final_flows_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flows_sample = flows_extract(flows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "# flows_sample.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "flows_sample.coalesce(1).write.csv('../Dataset/Output/total/flows_output_1.csv')\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combine Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "redteam_columns = ['time', 'src_comp', 'redteam_event']\n",
    "auth_columns = ['time', 'src_comp', 'fail_count', 'LoggedOn']\n",
    "dns_columns = ['time', 'src_comp', 'dns_count', 'newresolved_count']\n",
    "proc_columns = ['time', 'src_comp', 'proc_total', 'proc_exec_total', 'newexecute_count', 'Proc_run']\n",
    "flows_columns = ['time', 'src_comp', 'dur', 'pkt_cnt', 'byte_cnt', 'avg_pkt_size']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = \"../Dataset/Output/total/\"\n",
    "for root, dirs, files in os.walk(directory):\n",
    "    for file in files:\n",
    "        if file.endswith('.csv'):\n",
    "            filepath = root+'/'+file\n",
    "            if \"dns\" in root:   \n",
    "                pd_dns_df = pd.read_csv(filepath, names=dns_columns)\n",
    "                print(\"imported \", filepath)\n",
    "            elif \"proc\" in root:\n",
    "                pd_proc_df = pd.read_csv(filepath, names=proc_columns)\n",
    "                print(\"imported \", filepath)\n",
    "            elif \"flows\" in root:   \n",
    "                pd_flows_df = pd.read_csv(filepath, names=flows_columns)\n",
    "                print(\"imported \", filepath)\n",
    "            elif \"redteam\" in root:\n",
    "                pd_redteam_df = pd.read_csv(filepath, names=redteam_columns)\n",
    "                print(\"imported \", filepath)\n",
    "            elif \"auth\" in root:\n",
    "                pd_auth_df = pd.read_csv(filepath, names=auth_columns)\n",
    "                print(\"imported \", filepath)\n",
    "            else:\n",
    "                print(\"Error importing \", file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_size = pd_auth_df.time.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd_auth_df.time.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = '../Dataset/Output/complete_df.csv'\n",
    "\n",
    "splits = 15\n",
    "\n",
    "for n in range(int(splits)):\n",
    "\n",
    "    lower = int(n_size/splits*n)\n",
    "    upper = int(n_size/splits*(n+1))\n",
    "    \n",
    "    redteam = pd_redteam_df[(pd_redteam_df.time > lower) & (pd_redteam_df.time <= upper)]\n",
    "\n",
    "    dns = pd_dns_df[(pd_dns_df.time > lower) & (pd_dns_df.time <= upper)]\n",
    "\n",
    "    proc = pd_proc_df[(pd_proc_df.time > lower) & (pd_proc_df.time <= upper)]\n",
    "\n",
    "    flows = pd_flows_df[(pd_flows_df.time > lower) & (pd_flows_df.time <= upper)]\n",
    "\n",
    "    auth = pd_auth_df[(pd_auth_df.time > lower) & (pd_auth_df.time <= upper)]\n",
    "\n",
    "    master_df = dns.merge(auth,on=['time','src_comp'],how='outer')\n",
    "    master_df = master_df.merge(flows,on=['time','src_comp'],how='outer')\n",
    "    master_df = master_df.merge(redteam,on=['time','src_comp'],how='outer')\n",
    "    master_df = master_df.merge(proc,on=['time','src_comp'],how='outer')\n",
    "\n",
    "    master_df=master_df.fillna(0)\n",
    "\n",
    "    if not os.path.isfile(file):\n",
    "        master_df.to_csv(file)\n",
    "    else:\n",
    "        master_df.to_csv(file,mode='a',header=False )\n",
    "    print('Done with round ', (n+1),' of ',(splits), flush=True)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stringIndexer = StringIndexer(inputCol='dest_comp', outputCol='successType')\n",
    "# model = stringIndexer.fit(redteam)\n",
    "# indexed = model.transform(redteam)\n",
    "\n",
    "# encoder = OneHotEncoder(inputCol='successType', outputCol='successVec')\n",
    "# encoded = encoder.transform(indexed)\n",
    "# encoded.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section I tried transforming the dataframes into pandas dataframes. This works, sort of, but it is very slow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# redteam_pd_train = redteam_train.toPandas()\n",
    "# redteam_pd_train['time'] = pd.to_datetime(redteam_pd_train['time'],unit='s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # failed_logon_pd_train = failed_logon.toPandas()\n",
    "\n",
    "# # failed_user_logon_pd_train = failed_user_logon.toPandas()\n",
    "\n",
    "# failed_user_logon_pd_train['time'] = pd.to_datetime(failed_user_logon_pd_train['time'],unit='s')\n",
    "\n",
    "# failed_user_logon_pd_train_resamp = failed_user_logon_pd_train.groupby('src_user').apply(lambda x: x.set_index('time').resample('1Min').sum()).reset_index()\n",
    "\n",
    "# redteam_pd_user_train = redteam_pd_train.groupby('src_user').apply(lambda x: x.set_index('time').resample('1Min').sum()).reset_index()\n",
    "\n",
    "# combined_file = pd.merge(redteam_pd_train, failed_logon_pd_train, on=['time','src_comp'], how='outer')\n",
    "\n",
    "# inner_join = pd.merge(redteam_pd_train, failed_logon_pd_train, on=['time','src_comp'], how='inner')\n",
    "\n",
    "# inner_join.shape\n",
    "\n",
    "# inner_user_join = pd.merge(redteam_pd_train, failed_user_logon_pd_train, on=['time','src_user'], how='inner')\n",
    "\n",
    "# inner_user_join.shape\n",
    "\n",
    "# failed_logon_pd_train.shape\n",
    "\n",
    "# redteam_pd_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_subset(dataset, bat_size, n, start_time ):\n",
    "    return dataset.filter((dataset.time<((bat_size*(n+1))+start_time)) & (dataset.time>=(bat_size*n+start_time)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataset_extractor(redteam_dataset, authentication_dataset, flows_dataset, process_dataset, \n",
    "                     dns_dataset, iterations = 576, complete = False ,batch_size = 300 ):  \n",
    "    start_time = authentication_dataset.agg({\"time\":\"min\"}).collect()[0][\"min(time)\"]\n",
    "    total_time = authentication_dataset.agg({\"time\":\"max\"}).collect()[0][\"max(time)\"]\n",
    "    \n",
    "    if complete:\n",
    "        batches = int(total_time/batch_size)\n",
    "    else: \n",
    "        batches = iterations\n",
    "    switch = 0\n",
    "    \n",
    "    \n",
    "    flows_data = flows_extract(flows_dataset)\n",
    "    \n",
    "    dns_data = dns_extract(dns_dataset)\n",
    "\n",
    "    proc_data = proc_extract(process_dataset)\n",
    "    \n",
    "    proc_running_data = proc_running_extract(process_dataset)\n",
    "    \n",
    "    auth_data = auth_extract(authentication_dataset)\n",
    "    \n",
    "    auth_loggedon = auth_loggedon_extract(authentication_dataset)\n",
    "    \n",
    "    redteam_data = redteam_extract(redteam_dataset)\n",
    "    print(\"Data Extraction Complete\")\n",
    "    \n",
    "    for i in range(batches):\n",
    "        #Here we are breaking the data up into seperate segments\n",
    "        \n",
    "        subset_logon = time_subset(auth_data, batch_size, i, start_time)\n",
    "\n",
    "        subset_flows_data = time_subset(flows_data, batch_size, i, start_time)\n",
    "        \n",
    "#         subset_first_proc_data = time_subset(first_proc_data, batch_size, i, start_time)\n",
    "        subset_proc_data = time_subset(proc_data, batch_size, i, start_time)\n",
    "        subset_proc_running_data = time_subset(proc_running_data, batch_size, i, start_time)\n",
    "        subset_auth_loggedon_data = time_subset(auth_loggedon, batch_size, i, start_time)\n",
    "        \n",
    "        subset_dns_data_data = time_subset(dns_data, batch_size, i, start_time)\n",
    "#         subset_first_dns_data = time_subset(first_dns_data, batch_size, i, start_time)\n",
    "        \n",
    "        subset_redteam = time_subset(redteam_data, batch_size, i, start_time)\n",
    "        \n",
    "\n",
    "        #Here we join the data\n",
    "        temp_subset = subset_redteam.join(subset_logon,['time','src_comp'],'outer')\n",
    "        temp_subset = temp_subset.join(subset_flows_data,['time','src_comp'],'outer')\n",
    "#         temp_subset = temp_subset.join(subset_first_proc_data,['time','src_comp'],'outer')\n",
    "        temp_subset = temp_subset.join(subset_proc_data,['time','src_comp'],'outer')\n",
    "        temp_subset = temp_subset.join(subset_proc_running_data,['time','src_comp'],'outer')\n",
    "        temp_subset = temp_subset.join(subset_auth_loggedon_data,['time','src_comp'],'outer')\n",
    "        temp_subset = temp_subset.join(subset_dns_data_data,['time','src_comp'],'outer')\n",
    "#         temp_subset = temp_subset.join(subset_first_dns_data,['time','src_comp'],'outer')\n",
    "        \n",
    "        \n",
    "        temp_subset = temp_subset.na.fill(0)\n",
    "\n",
    "        temp_subset = temp_subset.groupby('src_comp').agg(F.min('time').alias('time')\\\n",
    "                                                         ,F.sum('redteam_event').alias('redteam_event')\\\n",
    "                                                         ,F.sum('dur').alias('dur')\\\n",
    "                                                         ,F.sum('fail_count').alias('fail_count')\\\n",
    "                                                         ,F.sum('dns_count').alias('dns_count')\\\n",
    "                                                         ,F.sum('newresolved_count').alias('newresolved_count')\\\n",
    "                                                         ,F.sum('proc_total').alias('proc_total')\\\n",
    "                                                         ,F.sum('Proc_run').alias('Proc_run')\\\n",
    "                                                         ,F.sum('LoggedOn').alias('LoggedOn')\\\n",
    "                                                         ,F.sum('proc_exec_total').alias('proc_exec_total')\\\n",
    "                                                         ,F.sum('newexecute_count').alias('newexecute_count')\\\n",
    "                                                         ,F.sum('pkt_cnt').alias('pkt_cnt')\\\n",
    "                                                         ,F.sum('byte_cnt').alias('byte_cnt')\\\n",
    "                                                         ,F.avg('avg_pkt_size').alias('avg_pkt_size'))\\\n",
    "                                                    .sort('time','src_comp')\n",
    "\n",
    "        temp_subset = temp_subset.withColumn('redteam_event',\\\n",
    "                                            F.when(temp_subset['redteam_event']>1,2).otherwise(temp_subset['redteam_event']))\n",
    "\n",
    "        if switch == 0:\n",
    "            master_subset = temp_subset\n",
    "            switch = 1\n",
    "        else:\n",
    "            master_subset = master_subset.union(temp_subset)\n",
    "    \n",
    "    return master_subset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def dataset_extractor(redteam_dataset, authentication_dataset, flows_dataset, process_dataset, \n",
    "#                      dns_dataset, iterations = 576, complete = False ,batch_size = 300 ):  \n",
    "#     start_time = authentication_dataset.agg({\"time\":\"min\"}).collect()[0][\"min(time)\"]\n",
    "#     total_time = authentication_dataset.agg({\"time\":\"max\"}).collect()[0][\"max(time)\"]\n",
    "    \n",
    "#     if complete:\n",
    "#         batches = int(total_time/batch_size)\n",
    "#     else: \n",
    "#         batches = iterations\n",
    "#     switch = 0\n",
    "    \n",
    "    \n",
    "#     flows_data = flows_extract(flows_dataset)\n",
    "    \n",
    "#     dns_data = dns_extract(dns_dataset)\n",
    "\n",
    "#     proc_data = proc_extract(process_dataset)\n",
    "    \n",
    "#     proc_running_data = proc_running_extract(process_dataset)\n",
    "    \n",
    "# #     auth_data = auth_extract(authentication_dataset)\n",
    "    \n",
    "# #     auth_loggedon = auth_loggedon_extract(authentication_dataset)\n",
    "    \n",
    "#     redteam_data = redteam_extract(redteam_dataset)\n",
    "#     print(\"Data Extraction Complete\")\n",
    "    \n",
    "#     for i in range(batches):\n",
    "#         #Here we are breaking the data up into seperate segments\n",
    "        \n",
    "# #         subset_logon = time_subset(auth_data, batch_size, i, start_time)\n",
    "\n",
    "#         subset_flows_data = time_subset(flows_data, batch_size, i, start_time)\n",
    "        \n",
    "# #         subset_first_proc_data = time_subset(first_proc_data, batch_size, i, start_time)\n",
    "#         subset_proc_data = time_subset(proc_data, batch_size, i, start_time)\n",
    "#         subset_proc_running_data = time_subset(proc_running_data, batch_size, i, start_time)\n",
    "# #         subset_auth_loggedon_data = time_subset(auth_loggedon, batch_size, i, start_time)\n",
    "        \n",
    "#         subset_dns_data_data = time_subset(dns_data, batch_size, i, start_time)\n",
    "# #         subset_first_dns_data = time_subset(first_dns_data, batch_size, i, start_time)\n",
    "        \n",
    "#         subset_redteam = time_subset(redteam_data, batch_size, i, start_time)\n",
    "        \n",
    "\n",
    "#         #Here we join the data\n",
    "#         temp_subset = subset_redteam.join(subset_flows_data,['time','src_comp'],'outer')\n",
    "# #         temp_subset = temp_subset.join(subset_flows_data,['time','src_comp'],'outer')\n",
    "# #         temp_subset = temp_subset.join(subset_first_proc_data,['time','src_comp'],'outer')\n",
    "#         temp_subset = temp_subset.join(subset_proc_data,['time','src_comp'],'outer')\n",
    "#         temp_subset = temp_subset.join(subset_proc_running_data,['time','src_comp'],'outer')\n",
    "# #         temp_subset = temp_subset.join(subset_auth_loggedon_data,['time','src_comp'],'outer')\n",
    "#         temp_subset = temp_subset.join(subset_dns_data_data,['time','src_comp'],'outer')\n",
    "# #         temp_subset = temp_subset.join(subset_first_dns_data,['time','src_comp'],'outer')\n",
    "        \n",
    "        \n",
    "#         temp_subset = temp_subset.na.fill(0)\n",
    "\n",
    "#         temp_subset = temp_subset.groupby('src_comp').agg(F.min('time').alias('time')\\\n",
    "#                                                          ,F.sum('redteam_event').alias('redteam_event')\\\n",
    "#                                                          ,F.sum('dur').alias('dur')\\\n",
    "#                                                          ,F.sum('dns_count').alias('dns_count')\\\n",
    "#                                                          ,F.sum('newresolved_count').alias('newresolved_count')\\\n",
    "#                                                          ,F.sum('proc_total').alias('proc_total')\\\n",
    "#                                                          ,F.sum('Proc_run').alias('Proc_run')\\\n",
    "#                                                          ,F.sum('proc_exec_total').alias('proc_exec_total')\\\n",
    "#                                                          ,F.sum('newexecute_count').alias('newexecute_count')\\\n",
    "#                                                          ,F.sum('pkt_cnt').alias('pkt_cnt')\\\n",
    "#                                                          ,F.sum('byte_cnt').alias('byte_cnt')\\\n",
    "#                                                          ,F.avg('avg_pkt_size').alias('avg_pkt_size'))\\\n",
    "#                                                     .sort('time','src_comp')\n",
    "\n",
    "#         temp_subset = temp_subset.withColumn('redteam_event',\\\n",
    "#                                             F.when(temp_subset['redteam_event']>1,2).otherwise(temp_subset['redteam_event']))\n",
    "\n",
    "#         if switch == 0:\n",
    "#             master_subset = temp_subset\n",
    "#             switch = 1\n",
    "#         else:\n",
    "#             master_subset = master_subset.union(temp_subset)\n",
    "    \n",
    "#     return master_subset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# master_subset = dataset_extractor(redteam_train, auth_train, flows_train, proc_train, dns_train)\n",
    "# print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "master_subset.coalesce(1).write.csv('../Dataset/Output/final_output.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['src_comp',\n",
       " 'time',\n",
       " 'redteam_event',\n",
       " 'dur',\n",
       " 'fail_count',\n",
       " 'dns_count',\n",
       " 'newresolved_count',\n",
       " 'proc_total',\n",
       " 'Proc_run',\n",
       " 'LoggedOn',\n",
       " 'proc_exec_total',\n",
       " 'newexecute_count',\n",
       " 'pkt_cnt',\n",
       " 'byte_cnt',\n",
       " 'avg_pkt_size']"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# master_subset.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Extraction Complete\n"
     ]
    }
   ],
   "source": [
    "test_master_subset = dataset_extractor(redteam_test, auth_test, flows_test, proc_test, dns_test, 288)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "master_subset.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# master_subset.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# label = master_subset.select('redteam_event').alias('label')\n",
    "# feature_labels = [c for c in master_subset.columns if c not in {'time','src_comp','redteam_event'}]\n",
    "# features = master_subset.select([c for c in master_subset.columns if c not in {'time','src_comp','redteam_event'}])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_labels = [c for c in master_subset.columns if c not in {'time','src_comp','redteam_event'}]\n",
    "# features = master_subset.select([c for c in master_subset.columns if c not in {'time','src_comp','redteam_event'}])\n",
    "assembler = VectorAssembler(inputCols=feature_labels, outputCol=\"features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vecAssembler = VectorAssembler(inputCols=feature_labels, outputCol=\"features\")\n",
    "# features = vecAssembler.transform(features).head().features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# features.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tester = label.withColumn('features',features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tester = master_subset.select('redteam_event','dur', 'fail_count', 'pkt_cnt', 'byte_cnt', 'avg_pkt_size')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create a LogisticRegression instance. This instance is an Estimator.\n",
    "# lr = LogisticRegression(maxIter=10, regParam=0.01)\n",
    "# # Print out the parameters, documentation, and any default values.\n",
    "# print(\"LogisticRegression parameters:\\n\" + lr.explainParams() + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model1 = lr.fit(tester)\n",
    "\n",
    "# # Since model1 is a Model (i.e., a transformer produced by an Estimator),\n",
    "# # we can view the parameters it used during fit().\n",
    "# # This prints the parameter (name: value) pairs, where names are unique IDs for this\n",
    "# # LogisticRegression instance.\n",
    "# print(\"Model 1 was fit using parameters: \")\n",
    "# print(model1.extractParamMap())\n",
    "\n",
    "# # We may alternatively specify parameters using a Python dictionary as a paramMap\n",
    "# paramMap = {lr.maxIter: 20}\n",
    "# paramMap[lr.maxIter] = 30  # Specify 1 Param, overwriting the original maxIter.\n",
    "# paramMap.update({lr.regParam: 0.1, lr.threshold: 0.55})  # Specify multiple Params.\n",
    "\n",
    "# # You can combine paramMaps, which are python dictionaries.\n",
    "# paramMap2 = {lr.probabilityCol: \"myProbability\"}  # Change output column name\n",
    "# paramMapCombined = paramMap.copy()\n",
    "# paramMapCombined.update(paramMap2)\n",
    "\n",
    "# # Now learn a new model using the paramMapCombined parameters.\n",
    "# # paramMapCombined overrides all parameters set earlier via lr.set* methods.\n",
    "# model2 = lr.fit(training, paramMapCombined)\n",
    "# print(\"Model 2 was fit using parameters: \")\n",
    "# print(model2.extractParamMap())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_reg_redteam = LogisticRegression(featuresCol = 'features', labelCol='redteam_event')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline(stages = [assembler,log_reg_redteam])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit_model = pipeline.fit(master_subset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# proc_domains = proc.select('domain').distinct()\n",
    "# proc_users = proc.select('user').distinct()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# proc_domains.coalesce(1).write.csv('domains.csv')\n",
    "# proc_users.coalesce(1).write.csv('users.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# proc.select(*(sum(col(c).isNull().cast(\"int\")).alias(c) for c in proc.columns)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# flows.select(*(sum(col(c).isNull().cast(\"int\")).alias(c) for c in flows.columns)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auth.select('success').distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auth.select(*(sum(col(c).isNull().cast(\"int\")).alias(c) for c in auth.columns)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flows.orderBy(\"avg_pkt_size\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flows.select([count(when(isnan(c)|col(c).isNull(), c)).alias(c) for c in flows.columns]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dns.select(*(sum(col(c).isNull().cast(\"int\")).alias(c) for c in dns.columns)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "proc.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "redteam.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import lit, unix_timestamp\n",
    "start = datetime.date(2018,1,1)\n",
    "#datetime.timestamp(2018,1,1,12,0,0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datetime.datetime.fromtimestamp(time.mktime(start.timetuple()) + 228150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "func = udf (lambda x: datetime.datetime.fromtimestamp(time.mktime(start.timetuple()) + x).date(),DateType() )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "redteam1 = redteam.withColumn(\"timestam\", redteam.select(\"time\")),'yyyy-MM-dd HH:mm:ss').cast(\"timestamp\") )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "time_update = udf(lambda x: time.strftime('%m/%d %H:%M:%S', time.gmtime(x)))\n",
    "\n",
    "#timestam = time.strftime('%m/%d %H:%M:%S', time.gmtime(redteam.select(\"time\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Classification model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = StratifiedKFold(n_splits=6)\n",
    "classifier = xgb.XGBClassifier()\n",
    "\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n",
    "\n",
    "\n",
    "tprs = []\n",
    "aucs = []\n",
    "mean_fpr = np.linspace(0, 1, 100)\n",
    "\n",
    "i = 0\n",
    "for train, test in cv.split(X, y):\n",
    "    print('round ',i+1)\n",
    "    probas_ = classifier.fit(X[train], y[train]).predict_proba(X[test])\n",
    "    # Compute ROC curve and area the curve\n",
    "    fpr, tpr, thresholds = roc_curve(y[test], probas_[:, 1])\n",
    "    tprs.append(interp(mean_fpr, fpr, tpr))\n",
    "    tprs[-1][0] = 0.0\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    aucs.append(roc_auc)\n",
    "    plt.plot(fpr, tpr, lw=1, alpha=0.3,\n",
    "             label='ROC fold %d (AUC = %0.2f)' % (i, roc_auc))\n",
    "\n",
    "    i += 1\n",
    "plt.plot([0, 1], [0, 1], linestyle='--', lw=2, color='r',\n",
    "         label='Chance', alpha=.8)\n",
    "\n",
    "mean_tpr = np.mean(tprs, axis=0)\n",
    "mean_tpr[-1] = 1.0\n",
    "mean_auc = auc(mean_fpr, mean_tpr)\n",
    "std_auc = np.std(aucs)\n",
    "plt.plot(mean_fpr, mean_tpr, color='b',\n",
    "         label=r'Mean ROC (AUC = %0.2f $\\pm$ %0.2f)' % (mean_auc, std_auc),\n",
    "         lw=2, alpha=.8)\n",
    "\n",
    "std_tpr = np.std(tprs, axis=0)\n",
    "tprs_upper = np.minimum(mean_tpr + std_tpr, 1)\n",
    "tprs_lower = np.maximum(mean_tpr - std_tpr, 0)\n",
    "plt.fill_between(mean_fpr, tprs_lower, tprs_upper, color='grey', alpha=.2,\n",
    "                 label=r'$\\pm$ 1 std. dev.')\n",
    "\n",
    "plt.xlim([-0.05, 1.05])\n",
    "plt.ylim([-0.05, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver operating characteristic example')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classify Test Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interpret Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
